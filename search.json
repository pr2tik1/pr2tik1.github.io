[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": " devExplore ",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nBuilding a URL shortner\n\n\n\n\n\n\nSystem Design\n\n\nPython\n\n\nREST API\n\n\nWeb Application\n\n\n\n\n\n\n\n\n\nSep 10, 2024\n\n\nPratik Kumar\n\n\n13 min\n\n\n\n\n\n\n\n\n\n\n\n\nSimplifying R Package Management - renv vs packrat\n\n\n\n\n\n\nR\n\n\nCoding Practices\n\n\n\n\n\n\n\n\n\nJan 19, 2024\n\n\nPratik Kumar\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nEfficient project management using Dockers\n\n\n\n\n\n\nDockers\n\n\nPython\n\n\nR\n\n\nCoding Practices\n\n\n\n\n\n\n\n\n\nJul 31, 2023\n\n\nPratik Kumar\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nHierarchical Data Visualization Demystified\n\n\n\n\n\n\nData Visualization\n\n\nPlotly\n\n\nPython\n\n\nCoding Practices\n\n\n\n\n\n\n\n\n\nJun 19, 2023\n\n\nPratik Kumar\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\nLoan Status Prediction\n\n\n\n\n\n\nData Science\n\n\nData Visualization\n\n\nPython\n\n\nMachine Learning\n\n\nFeature Engineering\n\n\nKaggle\n\n\n\n\n\n\n\n\n\nApr 25, 2021\n\n\nPratik Kumar\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\nTitanic Survival Prediction\n\n\n\n\n\n\nData Science\n\n\nData Visualization\n\n\nPython\n\n\nMachine Learning\n\n\nFeature Engineering\n\n\nKaggle\n\n\n\n\n\n\n\n\n\nMar 21, 2021\n\n\nPratik Kumar\n\n\n15 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/remote-extension/remote-containers.html",
    "href": "posts/remote-extension/remote-containers.html",
    "title": "Efficient project management using Dockers",
    "section": "",
    "text": "Dockers are excellent tools in software development. The key benefit of Dockers is that they allow users to package an application with all its dependencies into a standardized unit for software development. They have a low impact on the operating system and are very fast, using minimal disk space. Instead of encapsulating the entire machine, they encapsulate the environments for an application, making it easy to share code, rebuild applications, and distribute them.\nUsing Microsoft’s extension called : Dev Containers we can harness these benefits. The Dev Containers extension enables the use of a Docker container as a full-featured development environment. This helps developers in the following ways:\n\nDevelop with a consistent and reproducible environment.\nQuickly swap between different, separate development environments and safely make updates without worrying about impacting your local machine.\nMake it easy for new team members / contributors to get up and running in a consistent development environment.\n\nPersonally, I have been using Dev Containers to manage projects with different requirements. As a software engineer at Elucidata, I am responsible for developing various applications that require different versions of R, Python, and JavaScript. Remote containers have been immensely helpful in keeping the dedicated packages for each application within their project directories only.\nWhile there are other alternatives such as virtual environments (e.g.- pyenv, venv) and package management tools (for e.g. - packrat, renv), containerization proves to be more efficient for bigger projects where we have to manage these R and Python environments together. With minimal code, we can instruct the required installations and quickly reproduce the environment.\nThe Dev Containers extension supports two primary operating models:\n\nYou can use a container as your full-time development environment\nYou can attach to a running container to inspect it."
  },
  {
    "objectID": "posts/remote-extension/remote-containers.html#requirements",
    "href": "posts/remote-extension/remote-containers.html#requirements",
    "title": "Efficient project management using Dockers",
    "section": "Requirements",
    "text": "Requirements\nYou can use Docker with the Dev Containers extension in a few ways, including:\n\nDocker installed locally\nDocker installed on a remote environment\nOther Docker compliant CLIs, installed locally or remotely\n\nWhile other CLIs may work, they are not officially supported. Note that attaching to a Kubernetes cluster only requires a properly configured kubectl CLI\n\n\nVisit here to know more about system requirements."
  },
  {
    "objectID": "posts/remote-extension/remote-containers.html#installation",
    "href": "posts/remote-extension/remote-containers.html#installation",
    "title": "Efficient project management using Dockers",
    "section": "Installation",
    "text": "Installation\nTo get started, follow this official step by step tutorial here."
  },
  {
    "objectID": "posts/remote-extension/remote-containers.html#devcontainerdevcontainer.json",
    "href": "posts/remote-extension/remote-containers.html#devcontainerdevcontainer.json",
    "title": "Efficient project management using Dockers",
    "section": ".devcontainer/devcontainer.json",
    "text": ".devcontainer/devcontainer.json\nWithin the .devcontainer folder the devcontainer.json config file helps the extension to determine the name of the container, which image to use, extensions to install, port to expose and other configurations. Few examples,\n\n1. Shiny Application\nConsider there are multiple projects that you are working on, and each of them require different R versions and packages. You can set the value of the image key from below to any version of choice for R.\n{\n    \"name\" : \"Project 1\",\n    \"image\": \"r-base:latest\" //Any version as per requirement from dockerhub\n}\n{\n    \"name\" : \"Project 2\",\n    \"image\": \"r-base:4.1\" //Any version as per requirement from dockerhub\n}\n\n\n2. Using Custom Docker Image\nNow, suppose you want to use your own image with a specific R version, package installations, and other tools/languages included. In that case, you can create a custom Dockerfile (just make sure to specify the correct path for this file) as follows,\n# Use the specified R base image\nFROM r-base:latest\n\n# Set non-interactive mode for apt-get\nENV DEBIAN_FRONTEND=noninteractive\n\n# Expose port 4200\nEXPOSE 4200\n\n# Install necessary system dependencies\nRUN apt-get update && apt-get install -y \\\n    make \\\n    build-essential \\\n    git \\\n    wget \\\n    curl \n\n## Python setup\n# Switch to user \"docker\" to install pyenv\nUSER docker\nRUN curl https://pyenv.run | bash\n\n# Set up Python paths\nENV PATH=\"/home/docker/.pyenv/bin:$PATH\"\nENV PATH=\"/home/docker/.pyenv/versions/3.9.13/bin:$PATH\"\n\n# Install Python 3.9.13\nRUN PYTHON_CONFIGURE_OPTS=\"--enable-shared\" pyenv install 3.9.13\n\n# Switch back to root user for the remaining steps\nUSER root\n\n## Python virtual env and packages\n# Copy requirements.txt into the image\nCOPY requirements.txt requirements.txt\n\n# Create and activate Python virtual environment\nRUN rm -rf my_venv && python3 -m venv my_venv && \\\n    . my_venv/bin/activate && \\\n    python3 -m pip install --upgrade pip && \\\n    pip install -r requirements.txt\n\n# Install R packages using renv\nCOPY renv.lock renv.lock\nRUN R -e \"install.packages('renv'); renv::restore()\"\nNow with this Dockerfile, run the extension using following devcontainer.json :\n{\n    \"name\": \"Custom App\",\n    \"build\": {\n      \"dockerfile\": \"Dockerfile\",  //path to your custom Dockerfile.\n      \"context\": \"..\"\n    },\n    \"remoteUser\": \"docker\"\n}\n\n\n3. Post Installation Step\nWhen it comes to development, there could be a constant update in list of packages in your project. Now building everytime for each newly added package can be time consuming. Hence, we can install packages after building the image with minimal layers. So that the files would like :\n\nThe updated config file .devcontainer/devcontainer.json would now look like:\n\n{\n    \"name\": \"Custom App\",\n    \"build\": {\n      \"dockerfile\": \"Dockerfile\", //path to your custom Dockerfile.\n      \"context\": \"..\"\n    },\n    \"remoteUser\": \"docker\",\n    \"postCreateCommand\": \"bash .devcontainer/build_environment.sh\" // Can use any scripting language of choice.\n}\n\nYou can use a bash script to install packages after the build of container .devcontainer/build_environment.sh :\n\n## Install R packages\nR -e \"install.packages('renv'); renv::restore()\"\n\n\n## Install python packages\nrm -rf my_venv && python3 -m venv my_venv && \\\n    . my_venv/bin/activate && \\\n    python3 -m pip install --upgrade pip && \\\n    pip install -r requirements.txt"
  },
  {
    "objectID": "posts/kaggle/titanic-survival-eda-feature-engineering-preds.html",
    "href": "posts/kaggle/titanic-survival-eda-feature-engineering-preds.html",
    "title": "Titanic Survival Prediction",
    "section": "",
    "text": "Welcome to the Titanic Survival Prediction project, a classic challenge that serves as the perfect starting point for anyone looking to dive into the world of Machine Learning and data competitions on Kaggle. This project involves building a predictive model to determine which passengers survived the infamous Titanic disaster, a task that will guide you through essential steps in data science, from feature engineering to model development and evaluation.\nThis competition is designed to help you get comfortable with the Kaggle platform and machine learning workflows. You’ll be using Python to explore the data, perform feature engineering, visualize key trends, and develop a predictive model that can accurately classify survivors. For more details, visit the competition page and check out Kaggle’s YouTube video for a comprehensive introduction.\n\nImport Data: Load the Titanic dataset to begin the exploration and analysis.\nFeature Engineering: Transform raw data into meaningful features that improve model performance.\nData Visualization: Analyze and visualize the data to uncover patterns and insights.\nModel Development: Build and train machine learning models to predict passenger survival.\nModel Testing: Evaluate model accuracy and fine-tune parameters to optimize results.\nPrediction and Submission: Generate survival predictions and submit them to the Kaggle leaderboard.\n\nEmbark on this journey to not only enhance your data science skills but also understand the power of predictive modeling in real-world scenarios."
  },
  {
    "objectID": "posts/kaggle/titanic-survival-eda-feature-engineering-preds.html#introduction",
    "href": "posts/kaggle/titanic-survival-eda-feature-engineering-preds.html#introduction",
    "title": "Titanic Survival Prediction",
    "section": "",
    "text": "Welcome to the Titanic Survival Prediction project, a classic challenge that serves as the perfect starting point for anyone looking to dive into the world of Machine Learning and data competitions on Kaggle. This project involves building a predictive model to determine which passengers survived the infamous Titanic disaster, a task that will guide you through essential steps in data science, from feature engineering to model development and evaluation.\nThis competition is designed to help you get comfortable with the Kaggle platform and machine learning workflows. You’ll be using Python to explore the data, perform feature engineering, visualize key trends, and develop a predictive model that can accurately classify survivors. For more details, visit the competition page and check out Kaggle’s YouTube video for a comprehensive introduction.\n\nImport Data: Load the Titanic dataset to begin the exploration and analysis.\nFeature Engineering: Transform raw data into meaningful features that improve model performance.\nData Visualization: Analyze and visualize the data to uncover patterns and insights.\nModel Development: Build and train machine learning models to predict passenger survival.\nModel Testing: Evaluate model accuracy and fine-tune parameters to optimize results.\nPrediction and Submission: Generate survival predictions and submit them to the Kaggle leaderboard.\n\nEmbark on this journey to not only enhance your data science skills but also understand the power of predictive modeling in real-world scenarios."
  },
  {
    "objectID": "posts/kaggle/titanic-survival-eda-feature-engineering-preds.html#a.-import-data",
    "href": "posts/kaggle/titanic-survival-eda-feature-engineering-preds.html#a.-import-data",
    "title": "Titanic Survival Prediction",
    "section": "A. Import Data",
    "text": "A. Import Data\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\ntrain = pd.read_csv('../input/titanic/train.csv')\ntest = pd.read_csv('../input/titanic/test.csv')"
  },
  {
    "objectID": "posts/kaggle/titanic-survival-eda-feature-engineering-preds.html#b.-dataset-exploration",
    "href": "posts/kaggle/titanic-survival-eda-feature-engineering-preds.html#b.-dataset-exploration",
    "title": "Titanic Survival Prediction",
    "section": "B. Dataset exploration:",
    "text": "B. Dataset exploration:\n\n\ntrain.head()\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\n\n\n\n\nB.1. Types of Variables\n\n# Find categorical variables\ncategorical = [var for var in train.columns if train[var].dtype=='O']\nprint('There are {} categorical variables'.format(len(categorical)))\n\nThere are 5 categorical variables\n\n\n\n# Find numerical variables\nnumerical = [var for var in train.columns if train[var].dtype!='O']\nprint('There are {} numerical variables'.format(len(numerical)))\n\nThere are 7 numerical variables\n\n\n\nViewing the Categorical terms :\n\ndata = [train,test]\nfor dataset in data:\n    #Filter categorical variables\n    categorical_columns = [x for x in dataset.dtypes.index if dataset.dtypes[x]=='object']    \n    # Exclude ID cols and source:\n    categorical_columns = [x for x in categorical_columns if x not in ['PassengerId','Ticket','Name','Cabin']]\n    #Print frequency of categories\n    \nfor col in categorical_columns:\n    print ('\\nFrequency of Categories for variable %s'%col)\n    print (train[col].value_counts())\n\n\nFrequency of Categories for variable Sex\nSex\nmale      577\nfemale    314\nName: count, dtype: int64\n\nFrequency of Categories for variable Embarked\nEmbarked\nS    644\nC    168\nQ     77\nName: count, dtype: int64\n\n\n\n\n\nB.2. Detecting Missing Values\n\ntrain.isnull().sum()\n\nPassengerId      0\nSurvived         0\nPclass           0\nName             0\nSex              0\nAge            177\nSibSp            0\nParch            0\nTicket           0\nFare             0\nCabin          687\nEmbarked         2\ndtype: int64\n\n\n\ntrain.isnull().mean()\n\nPassengerId    0.000000\nSurvived       0.000000\nPclass         0.000000\nName           0.000000\nSex            0.000000\nAge            0.198653\nSibSp          0.000000\nParch          0.000000\nTicket         0.000000\nFare           0.000000\nCabin          0.771044\nEmbarked       0.002245\ndtype: float64\n\n\n\n\nMissing Data Overview\nThe train dataset has 12 features, with missing values observed in the following features:\n\nAge: Missing in 19.86% of the records\nCabin: Missing in 77.10% of the records\nEmbarked: Missing in 0.22% of the records\n\n\n\nAnalysis and Assumptions About Missing Data\n\nCabin\nThe Cabin feature has the highest proportion of missing values (77.10%). This substantial amount of missing data might suggest that:\n\nFor many individuals who did not survive, the cabin information was not recorded or available.\nSurvivors, on the other hand, may have been able to provide this information.\n\nThe missingness here could be due to the nature of the records or circumstances surrounding the individuals who did not survive, making this data likely to fall into the Missing Not At Random (MNAR) category. This means the missingness is related to the unobserved value itself or other factors not accounted for.\n\n\nAge\nThe Age feature has missing values in about 22% of the records. This could be due to:\n\nMissing age information for individuals who did not survive.\nSurvivors possibly being able to provide their age when asked.\n\nThis type of missing data might also be categorized as Missing Not At Random (MNAR) if the likelihood of missing data is related to whether the individual survived or other unobserved factors.\n\n\nEmbarked\nThe Embarked feature has a very small proportion of missing values (0.22%). This is a very minor amount and is likely due to random occurrences.\nSuch a small percentage of missing data is often considered Missing Completely At Random (MCAR), meaning the missingness is unrelated to the observed or unobserved data.\n\n\n\nSummary\n\nCabin and Age features likely fall into the MNAR category due to possible relationships between missingness and other factors like survival status.\nThe Embarked feature’s missing values are likely MCAR, as the missingness appears random and does not correlate with other data aspects.\n\n\n\nB.3. Outliers detection\n\nplt.figure(figsize=(8,6))\nplt.subplot(1, 2, 1)\nfig = train.boxplot(column='Age')\nfig.set_title('')\nfig.set_ylabel('Age')\n\nplt.subplot(1, 2, 2)\nfig = train.boxplot(column='Fare')\nfig.set_title('')\nfig.set_ylabel('Fare')\n\nText(0, 0.5, 'Fare')\n\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(8,6))\n\nplt.subplot(1, 2, 1)\nfig = train.Age.hist(bins=20)\nfig.set_ylabel('Number of passengers')\nfig.set_xlabel('Age')\n\nplt.subplot(1, 2, 2)\nfig = train.Fare.hist(bins=20)\nfig.set_ylabel('Number of passengers')\nfig.set_xlabel('Fare')\n\nText(0.5, 0, 'Fare')\n\n\n\n\n\n\n\n\n\n\n\nB.3. Analyzing the Embarked feature\n\ntrain[train.Embarked.isnull()]\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n61\n62\n1\n1\nIcard, Miss. Amelie\nfemale\n38.0\n0\n0\n113572\n80.0\nB28\nNaN\n\n\n829\n830\n1\n1\nStone, Mrs. George Nelson (Martha Evelyn)\nfemale\n62.0\n0\n0\n113572\n80.0\nB28\nNaN\n\n\n\n\n\n\n\nThe Embarked feature, which records the port of embarkation for passengers, has a very small proportion of missing values (0.22%). This low percentage of missing data suggests a specific pattern in how the data might be missing.\n\nPossible Reasons for Missing Values\n\nConsistency Among Passengers: For passengers who share the same ticket, cabin, and fare, it is unlikely that the missing Embarked data is due to discrepancies in their records. This is because passengers with identical ticket and cabin information would typically have consistent embarkation data.\nData Generation During Dataset Construction: The missing Embarked values could have resulted from data entry or construction processes. For example, if data was manually entered or generated, some records might have been incomplete due to errors or omissions during the data preparation phase.\n\n\n\nNature of Missing Data\nGiven that the missing values in the Embarked feature are minimal and appear to be random rather than systematic, we can categorize this missing data as:\n\nMissing Completely At Random (MCAR): The missingness of the Embarked data is likely unrelated to both the values of the Embarked feature itself and any other features in the dataset. The small percentage of missing data indicates that these omissions do not follow a discernible pattern and are likely due to random errors in data entry or processing.\n\nIn summary, the missing values in the Embarked feature are random and not indicative of any underlying patterns related to the data’s other aspects. This randomness supports the classification of this missing data as MCAR.\n\n\n\nB.4. Analyzing Cabin feature\n\ntrain['cabin_null'] = np.where(train.Cabin.isnull(),1,0)\ntrain.groupby(['Survived'])['cabin_null'].mean()\n\nSurvived\n0    0.876138\n1    0.602339\nName: cabin_null, dtype: float64\n\n\nThe above figures indicates that the missing data is more in the case of passengers not survived(=0).\nThere is a systematic loss of data: people who did not survive tend to have more information missing. Presumably, the method chosen to gather the information, contributes to the generation of these missing data.\n\n\nB.5. Analyzing the Age feature\n\ntrain['age_null'] = np.where(train.Age.isnull(),1,0)\ntrain.groupby(['Survived'])['age_null'].mean()\n\nSurvived\n0    0.227687\n1    0.152047\nName: age_null, dtype: float64\n\n\nThere is a systematic loss of data: people who did not survive tend to have more information missing. Presumably, the method chosen to gather the information, contributes to the generation of these missing data.\n\n\nB.6. Analyzing the Fare feature\nThe distribution of Fare is skewed, so in principle, we shouldn’t estimate outliers using the mean plus minus 3 standard deviations methods, which assumes a normal distribution of the data.\n\ntotal_passengers = float(train.shape[0])\n\nprint('Total number of passengers: {}'.format(train.shape[0]))\nprint('Passengers that paid more than 65: {:.2f}%'.format(\n    (train[train.Fare &gt; 65].shape[0] / total_passengers) * 100))\nprint('passengers that paid more than 100: {} %'.format((\n    train[train.Fare &gt; 100].shape[0]/ total_passengers)*100))\n\nTotal number of passengers: 891\nPassengers that paid more than 65: 13.02%\npassengers that paid more than 100: 5.948372615039282 %\n\n\nThere is unusual high values of Fares observed, the reason is found as follows:\n\n#at the most extreme outliers\ntrain[train.Fare&gt;300]\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\ncabin_null\nage_null\n\n\n\n\n258\n259\n1\n1\nWard, Miss. Anna\nfemale\n35.0\n0\n0\nPC 17755\n512.3292\nNaN\nC\n1\n0\n\n\n679\n680\n1\n1\nCardeza, Mr. Thomas Drake Martinez\nmale\n36.0\n0\n1\nPC 17755\n512.3292\nB51 B53 B55\nC\n0\n0\n\n\n737\n738\n1\n1\nLesurer, Mr. Gustave J\nmale\n35.0\n0\n0\nPC 17755\n512.3292\nB101\nC\n0\n0\n\n\n\n\n\n\n\nThese three people have the same ticket number, indicating that they were travelling together. The Fare price in this case, 512 is the price of 3 tickets, and not one. This is why, it is unusually high.\n\nB.7. Categorical Values :\n\nprint('Number of categories in the variable Name: {}'.format(\n    len(train.Name.unique())))\n\nprint('Number of categories in the variable Gender: {}'.format(\n    len(train.Sex.unique())))\n\nprint('Number of categories in the variable Ticket: {}'.format(\n    len(train.Ticket.unique())))\n\nprint('Number of categories in the variable Cabin: {}'.format(\n    len(train.Cabin.unique())))\n\nprint('Number of categories in the variable Embarked: {}'.format(\n    len(train.Embarked.unique())))\n\nprint('Total number of passengers in the Titanic: {}'.format(len(train)))\n\nNumber of categories in the variable Name: 891\nNumber of categories in the variable Gender: 2\nNumber of categories in the variable Ticket: 681\nNumber of categories in the variable Cabin: 148\nNumber of categories in the variable Embarked: 4\nTotal number of passengers in the Titanic: 891\n\n\n\ndrop_column = ['cabin_null','age_null']\ntrain.drop(drop_column , axis =1  ,inplace = True )"
  },
  {
    "objectID": "posts/kaggle/titanic-survival-eda-feature-engineering-preds.html#c.-feature-scaling-and-engineering",
    "href": "posts/kaggle/titanic-survival-eda-feature-engineering-preds.html#c.-feature-scaling-and-engineering",
    "title": "Titanic Survival Prediction",
    "section": "C. Feature Scaling and Engineering",
    "text": "C. Feature Scaling and Engineering\nFeature scaling is a technique used to standardize the range of independent variables or features of data. In machine learning and data analysis, scaling is important because it helps improve the performance and training stability of models.\n\nC.1. Handling the Missing Values:\nThe dataset contains missing values in several features. To address these, we apply different strategies based on the nature of each feature:\n\ntrain.isnull().sum()\n\nPassengerId      0\nSurvived         0\nPclass           0\nName             0\nSex              0\nAge            177\nSibSp            0\nParch            0\nTicket           0\nFare             0\nCabin          687\nEmbarked         2\ndtype: int64\n\n\n\ntest.isnull().sum()\n\nPassengerId      0\nPclass           0\nName             0\nSex              0\nAge             86\nSibSp            0\nParch            0\nTicket           0\nFare             1\nCabin          327\nEmbarked         0\ndtype: int64\n\n\n\ndata_cleaner = [test , train]\nfor dataset in data_cleaner:    \n    #completing missing age with median\n    dataset['Age'].fillna(dataset['Age'].median(), inplace = True)\n\n    #completing embarked with mode\n    dataset['Embarked'].fillna(dataset['Embarked'].mode()[0], inplace = True)\n\n    #completing missing fare with median\n    dataset['Fare'].fillna(dataset['Fare'].median(), inplace = True)\n    \n#delete the train feature\ntrain.drop(['Ticket'], axis=1, inplace = True)\ntest.drop(['Ticket'] , axis=1 , inplace = True)\n\n\n\nC.2. Encoding\nEncoding is a crucial step in data preprocessing, especially for machine learning and statistical modeling. It involves converting categorical variables (features that represent categories or groups) into numerical values that can be processed by machine learning algorithms.\n\nC.2.1. Cabin Feature\n\ndrop_column = ['Cabin']\ntrain.drop(drop_column , axis =1  ,inplace = True )\ntest.drop(drop_column , axis =1  ,inplace = True )\n\nThe Cabin feature has been dropped from the dataset due to its high proportion of missing values (77.10%), which makes it less informative.\n\n\nC.2.2. Fare Feature\n\nfull_data = [train,test]\nfor dataset in full_data:\n    dataset.loc[ dataset['Fare'] &lt;= 7.91, 'Fare_Band'] = 0\n    dataset.loc[(dataset['Fare'] &gt; 7.91) & (dataset['Fare'] &lt;= 14.454), 'Fare_Band'] = 1\n    dataset.loc[(dataset['Fare'] &gt; 14.454) & (dataset['Fare'] &lt;= 31), 'Fare_Band'] = 2\n    dataset.loc[ dataset['Fare'] &gt; 31, 'Fare_Band'] = 3\n    dataset['Fare_Band'] = dataset['Fare_Band'].astype(int)\n    dataset.drop(['Fare' ], axis = 1 , inplace =True)\n\nThe Fare feature has been transformed into discrete fare bands. This transformation categorizes fare amounts into bins, which can simplify the modeling process and potentially reveal patterns.\n\n\nC.2.3. Age Feature\n\nfull_data = [test , train]\nfor dataset in full_data:\n    \n    dataset.loc[ dataset['Age'] &lt;= 10, 'Age'] = 0\n    dataset.loc[(dataset['Age'] &gt; 10) & (dataset['Age'] &lt;= 15), 'Age'] = 1\n    dataset.loc[(dataset['Age'] &gt; 15) & (dataset['Age'] &lt;= 20), 'Age'] = 2\n    dataset.loc[(dataset['Age'] &gt; 20) & (dataset['Age'] &lt;= 25), 'Age'] = 3\n    dataset.loc[(dataset['Age'] &gt; 25) & (dataset['Age'] &lt;= 30), 'Age'] = 4\n    dataset.loc[(dataset['Age'] &gt; 30) & (dataset['Age'] &lt;= 45), 'Age'] = 5\n    dataset.loc[(dataset['Age'] &gt; 45) & (dataset['Age'] &lt;= 60), 'Age'] = 6\n    dataset.loc[ dataset['Age'] &gt; 60, 'Age'] = 7\n    dataset['Age'] = dataset['Age'].astype(int)\n\nThe Age feature has been converted into age bins, categorizing age into discrete intervals. This transformation simplifies the feature and can help capture age-related patterns more effectively.\n\n\nC.2.4. Sex and Embarked Feature\n\nfull_data = [test , train]\nfor dataset in full_data:\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n    dataset['Sex'] = dataset['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n\nThe categorical features Embarked and Sex have been encoded into numeric values. This encoding converts categorical variables into a format suitable for machine learning models.\n\n\nC.2.5. Droping the Name feature\n\ntrain.drop(['Name'],axis = 1, inplace = True)\ntest.drop(['Name'],axis = 1, inplace = True )\n\nThe Name feature, which does not provide useful information for modeling, has been removed from both the training and testing datasets.\n\n\nC.2.6. Family Size\n\ntrain['family_size'] = train['SibSp'] + train['Parch'] + 1 \ntest['family_size'] = test['SibSp'] + test['Parch'] + 1 \ntest['IsAlone'] = 1 \ntrain['IsAlone'] = 1 \ntrain['IsAlone'].loc[train['family_size'] &gt; 1] = 0\ntest['IsAlone'].loc[test['family_size'] &gt; 1] = 0 \ntest.drop(['SibSp' , 'Parch'], axis = 1 , inplace =True)\ntrain.drop(['SibSp','Parch' ], axis = 1 , inplace =True)\n\nA new feature, family_size, is created by combining SibSp (siblings/spouses aboard) and Parch (parents/children aboard). This feature provides insight into the size of the family traveling with the passenger.\n\ntest.isnull().sum()\n\nPassengerId    0\nPclass         0\nSex            0\nAge            0\nEmbarked       0\nFare_Band      0\nfamily_size    0\nIsAlone        0\ndtype: int64"
  },
  {
    "objectID": "posts/kaggle/titanic-survival-eda-feature-engineering-preds.html#d.-visualizations",
    "href": "posts/kaggle/titanic-survival-eda-feature-engineering-preds.html#d.-visualizations",
    "title": "Titanic Survival Prediction",
    "section": "D. Visualizations",
    "text": "D. Visualizations\n\nLet’s get some insights !\n\ng = sns.FacetGrid(train, col=\"Survived\", row=\"Sex\", hue=\"Embarked\", height=3)\ng.map(plt.hist, \"Pclass\", edgecolor=\"w\").add_legend()\n\n\n\n\n\n\n\n\n\nObservations\n\nFrom above graph we observe that more number of females survived as compared to males. The female survivors were more from the first class and male from third class were the most to die.\nThe 3rd class people were the most affected, that is they less survived where as 1st class people survived is maximum than others.\nThe second class has almost equal survived and couldn’t survive number of people. And also we notice many of the passengers Embarked from “S”.\n\n\n\nplt.figure(figsize = [8,5])\nsns.violinplot(x=\"Fare_Band\", y=\"Age\", data=train, hue='Survived',palette='coolwarm')\n\n\n\n\n\n\n\n\nMostly farebands are greater at the Age Group “4”. Survival also has greater area corresponding to age group “4”.\n\ntrain[['family_size', 'Survived']].groupby(['family_size'], as_index=False).mean()\n\n\n\n\n\n\n\n\nfamily_size\nSurvived\n\n\n\n\n0\n1\n0.303538\n\n\n1\n2\n0.552795\n\n\n2\n3\n0.578431\n\n\n3\n4\n0.724138\n\n\n4\n5\n0.200000\n\n\n5\n6\n0.136364\n\n\n6\n7\n0.333333\n\n\n7\n8\n0.000000\n\n\n8\n11\n0.000000\n\n\n\n\n\n\n\n\naxes = sns.catplot(x='family_size', y='Survived', hue='Sex', data=train, aspect=3, kind='point')\n\n\n\n\n\n\n\n\nWe find with increase in family size the survival rate decreases.\n\nplt.figure(figsize=(10,10))\nsns.heatmap(train.drop('PassengerId',axis=1).corr(), square=True, annot=True)\n\n\n\n\n\n\n\n\n\nUndestanding the Correlation matrix:\n\nThe FareBand and Pclass are highly correlated(-0.63) although negative, next to them is FareBand and IsAlone correlation(-0.57).\nThe Sex and Survived also have good correlation of (-0.54).\nBut as observed IsAlone and Family_size has the largest negative correlation (-0.69) is liable as the Family size and being alone are two opposite categories."
  },
  {
    "objectID": "posts/kaggle/titanic-survival-eda-feature-engineering-preds.html#e.-model-training-and-predicting",
    "href": "posts/kaggle/titanic-survival-eda-feature-engineering-preds.html#e.-model-training-and-predicting",
    "title": "Titanic Survival Prediction",
    "section": "E. Model Training and Predicting",
    "text": "E. Model Training and Predicting\n\nSpitting the data in ro train and test\n\nX = train.drop('Survived' , axis = 1 )\ny = train['Survived']\n\nfrom sklearn.model_selection import train_test_split\nX_train ,X_test , y_train , y_test = train_test_split(X , y , test_size = 0.3 , random_state =102)\n\nAlso we need to remove Id of passengers for prediction,\n\nX_train=X_train.drop(['PassengerId'],axis=1)\nX_test = X_test.drop(['PassengerId'],axis=1)\n\nImporting models from scikit learn module. The objective is to classify the passenger survivior into two classes: 0 or 1, hence this is a binary classification for which we will be using classifiers. Following part of this notebook compares and finds the best model suitable for the data based upon accuracy metrics.\n\n#Importing all models\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score\n\n\nE.1. Logistic Regression\n\nlogmodel = LogisticRegression()\nlogmodel.fit(X_train , y_train)\npred_l = logmodel.predict(X_test)\nacc_l = accuracy_score(y_test , pred_l)*100\nacc_l\n\n79.1044776119403\n\n\n\n\nE.2. Random Forest\n\nrandom_forest = RandomForestClassifier(n_estimators= 100)\nrandom_forest.fit(X_train, y_train)\npred_rf = random_forest.predict(X_test)\nacc_rf = accuracy_score(y_test , pred_rf)*100\nacc_rf\n\n83.2089552238806\n\n\n\n\nE.3. K-Nearest Neighbours\n\nknn = KNeighborsClassifier(n_neighbors = 3)\n\nknn.fit(X_train, y_train)\n\npred_knn = knn.predict(X_test)\n\nacc_knn = accuracy_score(y_test , pred_knn)*100\nacc_knn\n\n79.8507462686567\n\n\n\n\nE.4. Gaussian Naive Bayes Classifier\n\ngaussian = GaussianNB()\n\ngaussian.fit(X_train, y_train)\n\npred_gb = gaussian.predict(X_test)\n\nacc_gb = accuracy_score(y_test , pred_gb)*100\nacc_gb\n\n77.98507462686567\n\n\n\n\nE.5. C-Support Vector Classifier\n\nsvc = SVC()\n\nsvc.fit(X_train, y_train)\n\npred_svc = svc.predict(X_test)\n\nacc_svc = accuracy_score(y_test , pred_svc)*100\nacc_svc\n\n84.70149253731343\n\n\n\n\nE.6. Decision Tree\n\ndecision_tree = DecisionTreeClassifier()\n\ndecision_tree.fit(X_train, y_train)\n\npred_dt = decision_tree.predict(X_test)\n\nacc_dt = accuracy_score(y_test , pred_dt)*100\nacc_dt\n\n81.34328358208955\n\n\n\n\nE.7. Linear classifiers with SGD training.\n\nsgd = SGDClassifier()\n\nsgd.fit(X_train, y_train)\n\npred_sgd = sgd.predict(X_test)\n\nacc_sgd = accuracy_score(y_test , pred_sgd)*100\nacc_sgd\n\n73.13432835820896\n\n\n\n## Arranging the Accuracy results\nmodels = pd.DataFrame({\n    'Model': ['Logistic Regression', 'Random Forrest','K- Nearest Neighbour' ,\n             'Naive Bayes' , 'C-Support Vector Classifier' , 'Decision Tree' , 'Stochastic Gradient Descent'],\n    'Score': [acc_l , acc_rf , acc_knn , acc_gb , acc_svc , \n              acc_dt , acc_sgd]})\nmodels.sort_values(by='Score', ascending=False)\n\n\n\n\n\n\n\n\nModel\nScore\n\n\n\n\n4\nC-Support Vector Classifier\n84.701493\n\n\n1\nRandom Forrest\n83.208955\n\n\n5\nDecision Tree\n81.343284\n\n\n2\nK- Nearest Neighbour\n79.850746\n\n\n0\nLogistic Regression\n79.104478\n\n\n3\nNaive Bayes\n77.985075\n\n\n6\nStochastic Gradient Descent\n73.134328"
  },
  {
    "objectID": "posts/kaggle/titanic-survival-eda-feature-engineering-preds.html#ensemble-learning",
    "href": "posts/kaggle/titanic-survival-eda-feature-engineering-preds.html#ensemble-learning",
    "title": "Titanic Survival Prediction",
    "section": "Ensemble Learning",
    "text": "Ensemble Learning\n\ndf_test =  test.drop(['PassengerId'],axis=1)\n\np_l = logmodel.predict(df_test)\np_svc = svc.predict(df_test)\np_rf = random_forest.predict(df_test)\np_dt = decision_tree.predict(df_test)\n\n\npredict_combine = np.zeros((df_test.shape[0]))\nfor i in range(0, test.shape[0]):\n    temp = p_rf[i]+p_svc[i]+p_l[i]+p_dt[i]\n    if temp&gt;=2:\n        predict_combine[i] = 1\npredict_combine = predict_combine.astype('int')"
  },
  {
    "objectID": "posts/kaggle/titanic-survival-eda-feature-engineering-preds.html#submission",
    "href": "posts/kaggle/titanic-survival-eda-feature-engineering-preds.html#submission",
    "title": "Titanic Survival Prediction",
    "section": "Submission",
    "text": "Submission\n\nsubmission = pd.DataFrame({\n       \"PassengerId\": test[\"PassengerId\"],\n        \"Survived\": predict_combine\n    })\n\nsubmission.to_csv(\"submission.csv\", encoding='utf-8', index=False)\n\n\nsubmission.head()\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\n\n\n\n\n0\n892\n0\n\n\n1\n893\n0\n\n\n2\n894\n0\n\n\n3\n895\n0\n\n\n4\n896\n1"
  },
  {
    "objectID": "posts/r-package-mgt/r-package-mgt.html",
    "href": "posts/r-package-mgt/r-package-mgt.html",
    "title": "Simplifying R Package Management - renv vs packrat",
    "section": "",
    "text": "In this blog post, we’ll explore two popular tools for R package management – renv and packrat. We’ll delve into their features, advantages, and provide practical examples to help you make an informed decision."
  },
  {
    "objectID": "posts/r-package-mgt/r-package-mgt.html#the-conundrum-of-package-management",
    "href": "posts/r-package-mgt/r-package-mgt.html#the-conundrum-of-package-management",
    "title": "Simplifying R Package Management - renv vs packrat",
    "section": "The Conundrum of Package Management",
    "text": "The Conundrum of Package Management\nInstalling R packages is not always a walk in the park. Packages sourced from repositories such as CRAN, BiocManager, or GitHub can introduce complexity, involving multiple OS dependencies. This complexity magnifies during installation, making it challenging to manage, reproduce, and keep track of installed packages for a specific project. Enter package management tools, designed to streamline this process."
  },
  {
    "objectID": "posts/r-package-mgt/r-package-mgt.html#meet-renv",
    "href": "posts/r-package-mgt/r-package-mgt.html#meet-renv",
    "title": "Simplifying R Package Management - renv vs packrat",
    "section": "Meet renv",
    "text": "Meet renv\n\nWhat is renv?\nrenv is a package management tool for R that focuses on project-specific isolation. It allows you to create a self-contained environment for your R project, capturing and managing dependencies effectively.\n\n\nKey Features of renv\n\nIsolation: renv creates a dedicated project library, ensuring that each project has its own set of packages, eliminating conflicts between projects.\nSnapshotting: With renv, you can create a snapshot of your project’s dependencies, making it easy to replicate the environment on another machine.\nVersion Control Integration: renv integrates seamlessly with version control systems, ensuring reproducibility across different development environments.\n\n\n\nGetting Started with renv\n# Install renv\ninstall.packages(\"renv\")\n\n# Initialize renv in your project\nlibrary(renv)\nrenv::init()\n\n# Install and snapshot dependencies\nrenv::install(\"package_name\")\nrenv::snapshot()"
  },
  {
    "objectID": "posts/r-package-mgt/r-package-mgt.html#how-to-reproduce-renv-envrionment",
    "href": "posts/r-package-mgt/r-package-mgt.html#how-to-reproduce-renv-envrionment",
    "title": "Simplifying R Package Management - renv vs packrat",
    "section": "How to reproduce renv envrionment ?",
    "text": "How to reproduce renv envrionment ?\nThe above snapshot step produces a renv.lock file that is basically a record of the exact package versions and dependencies used in your project. This renv.lock file is your golden ticket to reproducing the renv environment on another machine or at a later time. Here’s how you can effortlessly replicate your renv environment:\n\nReproducing the renv Environment\n\nSharing the renv.lock File: Share the renv.lock file with your collaborators or store it in your version control system (e.g., Git). This file acts as a precise blueprint of your project’s dependencies.\nInitialization on a New Machine: On a new machine or for a different developer, start by cloning your project repository (if using version control). Navigate to the project directory and open an R session.\n\n# Install renv (if not already installed)\ninstall.packages(\"renv\")\n\n# Initialize renv in the project\nlibrary(renv)\nrenv::init()\n\n# Restore the environment using the lock file\nrenv::restore()\n\nInstalling Packages: Once the environment is restored, install the required packages using the snapshot.\n\n# Install packages from the lock file\nrenv::install()"
  },
  {
    "objectID": "posts/r-package-mgt/r-package-mgt.html#example-renv.lock-file",
    "href": "posts/r-package-mgt/r-package-mgt.html#example-renv.lock-file",
    "title": "Simplifying R Package Management - renv vs packrat",
    "section": "Example renv.lock file",
    "text": "Example renv.lock file\nIn the following example:\n\nThe “R” section specifies the R version used in the project.\nThe “Packages” section lists the packages used, each with its name, version, and source (e.g., CRAN).\nThe “Dependencies” section outlines the dependencies for each package, including both Imports and LinkingTo.\n\n{\n  \"R\": {\n    \"Version\": \"4.2.0\"\n  },\n  \"Packages\": {\n    \"data.table\": {\n      \"Package\": \"data.table\",\n      \"Version\": \"1.14.0\",\n      \"Source\": \"CRAN\"\n    },\n    \"ggplot2\": {\n      \"Package\": \"ggplot2\",\n      \"Version\": \"3.3.5\",\n      \"Source\": \"CRAN\"\n    }\n  },\n  \"Dependencies\": {\n    \"data.table\": {\n      \"Imports\": [],\n      \"LinkingTo\": []\n    },\n    \"ggplot2\": {\n      \"Imports\": [\n        \"methods\",\n        \"grDevices\",\n        \"graphics\",\n        \"stats\"\n      ],\n      \"LinkingTo\": []\n    }\n  }\n}"
  },
  {
    "objectID": "posts/r-package-mgt/r-package-mgt.html#exploring-packrat",
    "href": "posts/r-package-mgt/r-package-mgt.html#exploring-packrat",
    "title": "Simplifying R Package Management - renv vs packrat",
    "section": "Exploring packrat",
    "text": "Exploring packrat\n\nWhat is packrat?\npackrat is another robust package management tool for R. It addresses the challenges of package management by providing project-specific libraries and snapshot capabilities.\n\n\nKey Features of packrat\n\nBundled Library: packrat creates a local library for each project, ensuring that packages are self-contained within the project directory.\nSnapshotting: Similar to renv, packrat enables you to create a snapshot of your project’s dependencies, promoting reproducibility.\nIntegration with IDEs: packrat integrates seamlessly with popular R IDEs, making it convenient for developers who rely on specific development environments.\n\n\n\nGetting Started with packrat\n# Install packrat\ninstall.packages(\"packrat\")\n\n# Initialize packrat in your project\nlibrary(packrat)\npackrat::init()\n\n# Install and snapshot dependencies\npackrat::install(\"package_name\")\npackrat::snapshot()"
  },
  {
    "objectID": "posts/r-package-mgt/r-package-mgt.html#choosing-the-right-tool-for-you",
    "href": "posts/r-package-mgt/r-package-mgt.html#choosing-the-right-tool-for-you",
    "title": "Simplifying R Package Management - renv vs packrat",
    "section": "Choosing the Right Tool for You",
    "text": "Choosing the Right Tool for You\nBoth renv and packrat offer powerful solutions to the challenges of R package management. Your choice may depend on personal preference, project requirements, or team workflows. Experiment with both tools and consider factors like ease of use, integration capabilities, and community support.\nEffective package management is the cornerstone of reproducibility and collaboration in R development. Choose wisely, and may your R projects flourish without the headaches of package chaos!"
  },
  {
    "objectID": "posts/r-package-mgt/r-package-mgt.html#references",
    "href": "posts/r-package-mgt/r-package-mgt.html#references",
    "title": "Simplifying R Package Management - renv vs packrat",
    "section": "References",
    "text": "References\n\nrenv\npackrat\nrenv vs packrat"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Pratik Kumar",
    "section": "",
    "text": "Hello! 👋 I am a software engineer who loves working on websites, AI, and data-related projects. When I’m not coding, I enjoy going on treks, exploring new cuisines, and playing sports like football, badminton, cricket, and table tennis. I love photography and watching exciting movies like “Edge of Tomorrow”. And I am always up for a coffee !\nCurrently, I am working at Elucidata, solving some of the complex problems in drug discovery.\nView my resume here."
  },
  {
    "objectID": "about.html#work-experience",
    "href": "about.html#work-experience",
    "title": "Pratik Kumar",
    "section": "👨🏻‍💻 Work Experience",
    "text": "👨🏻‍💻 Work Experience\n\nElucidata | Aug 2021 - Present  - Software Engineer (current)  - Data Analyst\nElucidata | May 2021 - Aug 2021  - Data Science Intern\nBayer Cropscience | May 2019 - June 2019  - Data Science Intern"
  },
  {
    "objectID": "posts/system-design/short-url.html",
    "href": "posts/system-design/short-url.html",
    "title": "Building a URL shortner",
    "section": "",
    "text": "Designing a URL shortener, like TinyURL, is a classic problem in system design that offers a great opportunity to explore key design concepts. It’s not just about shrinking URLs—there’s a lot more going on behind the scenes.\nIn this blog, I’m excited to share how I approached this problem. I’ll walk you through my thought process, the challenges I faced, and the design decisions I made to turn long URLs into short, manageable links."
  },
  {
    "objectID": "posts/system-design/short-url.html#a.-introduction",
    "href": "posts/system-design/short-url.html#a.-introduction",
    "title": "Building a URL shortner",
    "section": "",
    "text": "Designing a URL shortener, like TinyURL, is a classic problem in system design that offers a great opportunity to explore key design concepts. It’s not just about shrinking URLs—there’s a lot more going on behind the scenes.\nIn this blog, I’m excited to share how I approached this problem. I’ll walk you through my thought process, the challenges I faced, and the design decisions I made to turn long URLs into short, manageable links."
  },
  {
    "objectID": "posts/system-design/short-url.html#b.-understanding-the-problem-and-establishing-design-scope",
    "href": "posts/system-design/short-url.html#b.-understanding-the-problem-and-establishing-design-scope",
    "title": "Building a URL shortner",
    "section": "B. Understanding the Problem and Establishing Design Scope",
    "text": "B. Understanding the Problem and Establishing Design Scope\nBefore diving into the design, let’s clarify the requirements of a URL Shortener service—what it does, why it’s needed, and who benefits from it.\nA URL shortener takes a long URL and converts it into a shorter, more manageable version. The primary goal is to create a compact web address that’s easier to share, remember, and track. This service typically shortens the URL by adding a random sequence of letters and numbers. For example, a URL shortener can transform a lengthy URL like:\n\nhttps://www.amazon.in/gp/product/B0CHX1W1XY/ref=s9_bw_cg_Budget_2f1_w\n\ninto a much shorter one like:\n\nhttps://tinyurl.com/ypbd82wy\n\n\nB.1. Scope and Assumptions\nNow that we have a clear understanding of what a URL Shortener is, let’s delve deeper into the specifics and outline the requirements. To do this, we’ll address the following questions:\n\nWhat are the key functional features required for this service?\n\nThe service should convert a long URL into a short one, and when the shortened URL is accessed, it should redirect the user to the original long URL.\n\nWhat is the expected duration of the service’s operation?\n\nFor the purpose of this design, we’ll assume the service will run for the next 10 years.\n\nHow many URLs will the system need to handle simultaneously?\n\nWe’ll assume the system may need to manage up to 1,000 URLs concurrently.\n\nWhat characters will be used in the shortened URLs?\n\nThe shortened URLs will use alphanumeric characters.\n\nHow long should the shortened URLs be?\n\nThe URLs should be kept as short as possible while maintaining uniqueness.\n\nWhat should happen if two or more users submit the same long URL?\n\nInitially, the same short URL can be returned for identical long URLs. In the future, we can incorporate analytics or additional metadata to create user-specific shortened URLs.\n\n\n\n\nB.2. System Capacity\nNext, we calculate the system’s capacity to ensure it can handle the required load. Given the need to manage around 1,000 URLs concurrently over a period of 10 years, we can estimate the total number of URLs the service will need to handle. The calculation is as follows:\n\n⇒ 60 seconds x 60 minutes x 24 hours x 365 days x 10 years x 1,000 URLs = 315 billion URLs over 10 years\n\nThis means the system needs to generate at least 315 billion unique URLs over the course of 10 years using alphanumeric short URLs. To determine how many characters are needed to support this capacity, we can calculate:\n\n⇒ Alphanumeric characters: a-z (26) + A-Z (26) + 0-9 (10) = 62 possible combinations\n\n\n⇒ Since 62^7 = 3.5 trillion, using 7 characters will provide more than enough combinations to handle the required number of URLs over the next 10 years.\n\nFor each URL, let’s estimate the space required:\n\nShortened URL: 7 bytes (1-2 bytes per character)\nLong URL: Assume around 100 bytes\nMetadata: 500 bytes (e.g., userId, isValid status)\n\nThis totals approximately 1,000 bytes per URL. Given the need to store 315 billion URLs over 10 years:\n\n⇒ 1,000 bytes x 315 billion URLs = 315 TB of storage space required."
  },
  {
    "objectID": "posts/system-design/short-url.html#c.-deep-dive-into-designing",
    "href": "posts/system-design/short-url.html#c.-deep-dive-into-designing",
    "title": "Building a URL shortner",
    "section": "C. Deep-Dive into designing",
    "text": "C. Deep-Dive into designing\n\n\n\n\n\n\nC.1. Client (Frontend/User Interface)\nThe frontend is where users interact with the URL shortener. It’s the part of the application that makes shortening URLs easy and intuitive. The challenge here is to design a user interface (UI) that is both simple and powerful without unnecessary complexities.\n\nC.1.1. Frontend Design: Simplified for Usability\n\nBasic Form – The Core Interaction\nEvery URL shortener needs a form where users can input their long URLs. But simplicity is key. A clean, minimal form with just one field (for the long URL) and a button (to generate the short link) reduces cognitive load for users. Users don’t want to be bombarded with options—just a quick, easy solution to shorten their URLs.\nOptional Tracking – Adding Value with Insights\nSome users may want more than just shortened URLs. Adding a feature for tracking how many times each short URL was clicked gives users insights into how their links are performing. This feature could be optional, visible only to users who need it. By offering advanced analytics, we provide added value to the core functionality.\n\nTech Stack Consideration: For the frontend, using modern frameworks like React or Vue.js allows for a fast, responsive interface. These frameworks also make it easier to scale and add new features as the project evolves. We can also utilise Tailwind.css or shadcn-ui to give a minimalistic look.\n\n\n\nFronted of the URL Shortener App (https://github.com/pr2tik1/url-shortner-service)\n\n\n\n\n\nC.2. Server (Backend/API)\nThe backend is where the heavy lifting happens. It processes the logic, manages the data, and ensures that requests are handled efficiently. The decisions made here directly impact the system’s scalability and performance.\n\nC.2.1. Core API Functions: The Building Blocks\n\nShorten URL – Creating Unique Short Links\nThe core functionality of the backend is to accept long URLs from the frontend, generate unique short URLs, and store this mapping in a database. There are different ways to generate short URLs. You could use hashing algorithms (e.g., MD5 or SHA-256) or a counter-based system that assigns a unique ID for each URL and encodes it (e.g., using Base62).\nHashing ensures that similar URLs produce distinct results, while a counter-based system can be faster but may require additional collision checks. Both approaches scale well but differ in complexity and performance.\nRedirect URL – Efficient Lookup and Redirection\nWhen a user accesses a short URL, the backend retrieves the corresponding long URL from the database and redirects the user. This operation needs to be lightning-fast to avoid any latency in the user experience. By using indexed database queries and possibly caching frequently accessed short URLs, we can ensure efficient lookups and near-instantaneous redirection.\nOptimizing database lookups (e.g., through caching) and ensuring that the redirection process is swift prevents slowdowns and improves the overall user experience.\nAnalytics API (Optional) – Tracking Engagement\nIf you want to give users the ability to track how often their links are used, an analytics API can be added to collect data such as click counts, user demographics, or referral sources. Implementing this feature requires balancing between data collection and performance, ensuring that analytics don’t slow down the core functionality.\nOffering analytics as an optional feature allows users to gain valuable insights into their links’ performance, adding another dimension to the service.\n\nTech Stack Consideration: For the backend, using a highly scalable and fast server-side language like Node.js, Python, or Go is ideal. These languages have great frameworks (like Express.js for Node.js or Flask for Python) that handle API requests efficiently.\nHere are the API endpoints that can be created to support these feature:\n\n\n\n\n\n\n\nC.2.2. Backend Logic : How It All Works Together\n\n\n\n\n\n\nURL Shortening Process\nWhen a user submits a long URL, the backend generates a unique short link. This could be achieved through hashing or by assigning a unique ID. The URL pair is then stored in the database. A system like MongoDB or MySQL can be used, depending on the size and complexity of the project. NoSQL databases like MongoDB are great for flexibility and scalability, especially if you expect to store billions of URLs.\n\n\n\n\n\n\n\nRedirection Process\nUpon accessing a short URL, the backend retrieves the original URL from the database and redirects the user. If a caching layer (like Redis) is used, popular URLs can be served even faster, reducing the load on the database and improving response times.\n\n\n\n\n\n\n\nAnalytics (Optional)\nIf analytics are enabled, the backend can track each visit to the short URL. Data such as the number of clicks, user details, and timestamps can be stored for further analysis. However, it’s essential to balance this tracking to avoid impacting performance.\n\nThis backend design ensures that the service is scalable, fast, and capable of handling high volumes of traffic with optional analytics support.\n\n\n\nC.3. Load Balancer\nAs the popularity of our URL shortener grows, managing increased traffic becomes critical to avoid downtime and ensure seamless user experience. A load balancer is a key component that distributes incoming traffic across multiple servers, ensuring no single server bears too much load, which could lead to crashes or performance degradation.\nStrategy: To evenly distribute traffic, we can use algorithms like Round-Robin or Least Connections. Round-Robin cycles through servers in a balanced way, while Least Connections ensures that servers with fewer active requests receive more traffic, optimizing overall performance.\nTech Stack: Tools like Nginx or AWS Elastic Load Balancing are well-suited for handling traffic distribution across multiple servers efficiently.\nImplementing load balancing ensures higher availability and prevents bottlenecks by spreading out the traffic, making sure no single server is overwhelmed. This not only improves reliability but also optimizes resource utilization, especially as traffic scales.\n\n\nC.4. Caching Layer\nFrequent database lookups can slow down response times, especially as the user base grows. A caching layer helps mitigate this by storing frequently accessed URL mappings in memory, allowing the system to retrieve data much faster than querying the database each time.\nStrategy: The caching layer should prioritize storing the most popular URL mappings. By using policies like Least Recently Used (LRU), the cache remains efficient, keeping the most relevant data available in memory while discarding outdated information.\nTech Stack: Tools like Redis or Memcached are excellent choices for building a robust caching layer. Both provide high-speed, in-memory data storage that significantly reduces the load on the database and improves response times.\nCaching reduces the need for repeated database queries, enhancing the performance of frequently accessed URLs and providing a smoother user experience. By alleviating pressure on the database, the system scales more effectively without sacrificing speed.\n\n\nC.5. Database (Storage)\nTo support millions, or even billions, of URL mappings, a well-designed and scalable database is essential. The database must efficiently store, query, and update URL mappings, ensuring the system can handle high volumes of both reads and writes.\nSchema Design: The database schema should store short URL mappings alongside their original long URLs, user-related metadata, and expiration details (if applicable). Key fields include:\n\nshort_url: The unique identifier for the shortened URL.\nlong_url: The original URL.\nuser_id (optional): To track ownership if needed.\nis_valid: Indicates if the shortened URL is still active.\ncreated_at, expires_at: For tracking URL expiration or time-sensitive URLs.\n\nScalability: Choosing the right database is crucial for handling large-scale operations. MySQL works well for structured data and is suitable for smaller applications. However, MongoDB (NoSQL) is often a better choice for larger, more flexible applications that need to scale horizontally, especially when dealing with unstructured or rapidly growing data.\nSharding and Replication: As the system grows, sharding (dividing the database into smaller, more manageable parts) and replication (duplicating data across multiple servers) become essential for maintaining high performance and availability. These techniques distribute the load, ensuring the system remains responsive as it scales.\nBy designing a scalable and efficient database schema, you ensure the system can handle growth without experiencing performance bottlenecks. Proper sharding and replication strategies help maintain fast read and write operations, which is critical for any high-traffic application.\n\n\nC.6. URL Generation Algorithm\nGenerating unique and reliable short URLs is the core of any URL shortener. The right algorithm ensures that each URL gets a distinct short link without clashes.\n\nHash-Based Approach: One method is using a hashing algorithm like MD5 or SHA-256 to generate a unique short link for each URL. Hashing is useful because it consistently generates distinct outputs, even for similar inputs, making it ideal for creating unique URLs.\n\n\n\nCounter-Based Approach: Alternatively, a counter-based system can assign a unique ID to each new URL. The ID is then encoded (e.g., using Base62) to generate a short URL. While this method is faster, it requires careful tracking to avoid collisions and may involve a little more overhead in maintaining uniqueness.\n\n\n\nCollision Handling: Regardless of the approach, handling potential collisions (two URLs generating the same short link) is essential. If a collision is detected, a strategy like rehashing the URL or incrementing the counter can be used to generate a new, unique short link.\n\nBy selecting the right URL generation strategy and preparing for potential collisions, you ensure that the system consistently generates unique short links without errors. This step is crucial for maintaining data integrity and preventing conflicts within the system."
  },
  {
    "objectID": "posts/system-design/short-url.html#d.-challenges-and-trade-offs",
    "href": "posts/system-design/short-url.html#d.-challenges-and-trade-offs",
    "title": "Building a URL shortner",
    "section": "D. Challenges and Trade-offs",
    "text": "D. Challenges and Trade-offs\nBuilding a URL shortener involves several challenges and trade-offs that need to be carefully managed to ensure efficiency, scalability, and reliability:\n\n1. Scalability\nAs traffic grows, handling it effectively requires strategies like load balancing, caching, and database sharding. These ensure that the system can handle increased requests without performance bottlenecks or downtime.\n\n\n2. Database Choice\nChoosing between SQL and NoSQL is critical. SQL databases provide structure and consistency but may struggle under heavy loads. NoSQL databases, like MongoDB, scale better for high traffic but may compromise on data consistency, making the choice a balance between structure and scalability.\n\n\n3. Collision Handling\nIn hash or counter-based URL generation, the risk of collisions—where two URLs generate the same short link—must be addressed. The trade-off here is between speed and ensuring uniqueness, as more reliable methods may be slower.\n\n\n4. Caching\nCaching frequently accessed URLs helps improve response times, but improper cache management can lead to outdated or stale data. Implementing policies like Least Recently Used (LRU) helps strike a balance between performance and data freshness.\n\n\n5. Redirection Speed\nTo ensure users are quickly redirected, optimizing redirection speed through in-memory caching solutions like Redis is crucial. However, managing memory efficiently becomes a challenge as the system scales.\n\n\n6. Security\nPreventing malicious use of shortened URLs adds complexity. Implementing URL filtering and validation checks ensures security but may slow down the overall process.\n\n\n7. URL Expiration & Analytics\nHandling expiring URLs adds complexity to the database and requires scheduled cleanup. Additionally, tracking analytics like clicks and user data provides value but increases storage and processing demands, potentially affecting overall performance.\nEffectively balancing these factors allows the URL shortener to be scalable, fast, and secure, while still providing a smooth user experience."
  },
  {
    "objectID": "posts/system-design/short-url.html#e.-conclusion",
    "href": "posts/system-design/short-url.html#e.-conclusion",
    "title": "Building a URL shortner",
    "section": "E. Conclusion",
    "text": "E. Conclusion\nIn conclusion, designing a URL shortener system involves much more than just converting long URLs into shorter ones. The process requires careful consideration of system scalability, efficient URL storage and retrieval, and the potential for additional features like analytics and custom URL generation. By tackling challenges like efficient database querying, implementing caching layers, and balancing server loads, a robust, scalable service can be created that meets user needs while maintaining performance.\nI hope this exploration has offered useful strategies and insights. Thank you !\n\nReferences and Resources\n\nhttps://en.wikipedia.org/wiki/TinyURL\nApplication : https://github.com/pr2tik1/url-shortner-service\nhttps://stackoverflow.blog/2020/03/02/best-practices-for-rest-api-design/\nhttps://www.freecodecamp.org/news/rest-api-best-practices-rest-endpoint-design-examples/\nDesigns generated on : https://app.eraser.io/"
  },
  {
    "objectID": "posts/hierarchical/Plotly-Sunburst.html",
    "href": "posts/hierarchical/Plotly-Sunburst.html",
    "title": "Hierarchical Data Visualization Demystified",
    "section": "",
    "text": "Data visualization plays a vital role in various domains such as data analytics, data science, data dashboarding, and exploratory/statistical analysis. Within the Python and R ecosystems, there are several popular visualization libraries commonly used such as :\n\nMatplotlib\nSeaborn\nPlotly\nAltair\nBokeh\n\nAmong these, the widely used library is the Plotly Graphing Library, which offers libraries in multiple languages, high-quality scientific/non-scientific graphs, and easily shareable interactive plots.\nIn this post, I will be discussing an intriguing plot called the Sunburst Chart. Sunburst charts provide an interactive visualization of layered information, allowing for an enhanced understanding of complex data structures."
  },
  {
    "objectID": "posts/hierarchical/Plotly-Sunburst.html#introduction",
    "href": "posts/hierarchical/Plotly-Sunburst.html#introduction",
    "title": "Hierarchical Data Visualization Demystified",
    "section": "",
    "text": "Data visualization plays a vital role in various domains such as data analytics, data science, data dashboarding, and exploratory/statistical analysis. Within the Python and R ecosystems, there are several popular visualization libraries commonly used such as :\n\nMatplotlib\nSeaborn\nPlotly\nAltair\nBokeh\n\nAmong these, the widely used library is the Plotly Graphing Library, which offers libraries in multiple languages, high-quality scientific/non-scientific graphs, and easily shareable interactive plots.\nIn this post, I will be discussing an intriguing plot called the Sunburst Chart. Sunburst charts provide an interactive visualization of layered information, allowing for an enhanced understanding of complex data structures."
  },
  {
    "objectID": "posts/hierarchical/Plotly-Sunburst.html#sunburst-chart",
    "href": "posts/hierarchical/Plotly-Sunburst.html#sunburst-chart",
    "title": "Hierarchical Data Visualization Demystified",
    "section": "Sunburst Chart",
    "text": "Sunburst Chart\nA sunburst chart is a powerful visualization tool used to represent hierarchical datasets. In a hierarchical dataset, there exists a parent-child relationship among the features or variables, resembling a tree-like structure. To generate a sunburst plot using Plotly, you can leverage the capabilities of either plotly.express or plotly.graph_objects libraries.\nNow, let’s delve into how this data would appear by visualizing it using a sunburst chart."
  },
  {
    "objectID": "posts/hierarchical/Plotly-Sunburst.html#hierarchical-data",
    "href": "posts/hierarchical/Plotly-Sunburst.html#hierarchical-data",
    "title": "Hierarchical Data Visualization Demystified",
    "section": "Hierarchical Data",
    "text": "Hierarchical Data\nHierarchical datasets are a type of data organization where the data is structured in a hierarchical manner, forming a tree-like structure. In this structure, data elements are grouped into parent-child relationships, where each parent can have one or more children, and each child can be a parent of other elements, forming multiple levels of nesting.\nConsider an example dataframe (dummy data for demonstration purposes) with a tree-like structure, where the columns or features exhibit parent-child relationships with other columns.\n\nGeneral Dataset: This dataframe contains classes and values organized in columns, as depicted in the sample data provided.\nSunburst Dataset: This hierarchical dataframe defines the logical parent-child relationships between columns and their corresponding values.\n\nThe following dataset is a dummy data for demonstration.\n\n#Importing pandas to handle dataframe\nimport pandas as pd\n# Suppress pandas warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\ndata = pd.read_csv(\"../data/dummy_data.csv\")\ndata.head(5)\n\n\n\n\n\n\n\n\nCountry\nState\nCity\nPopulation\n\n\n\n\n0\nIndia\nINMP\nA1\n512\n\n\n1\nIndia\nINCG\nB2\n12201\n\n\n2\nIndia\nINCG\nM1\n9021\n\n\n3\nUSA\nUSNY\nC2\n812\n\n\n4\nUSA\nUSNY\nN1\n821\n\n\n\n\n\n\n\nThe dataset is not in hierachical form. The sunburst chart needs a parent, child and value variable for generating the plot. Hence, we need to convert the table into a ‘chart-acceptable’ format. The following function performs the job. The function is modified version of original function defined at Plotly’s documentation, to know more about this please visit here.\n\ndef build_hierarchical_dataframe(df, levels, value_column, metric):\n    \"\"\"\n    Build a hierarchy of levels for Sunburst.\n    \n    Args:\n        df (pd.DataFrame): Input pandas DataFrame.\n        levels (list): List of column names in the order, child to root.\n        value_column (str): Name of the column to display in the chart.\n        metric (str): Specifies the metric, either \"sum\" or \"count\".\n        \n    Returns:\n        pd.DataFrame: A pandas DataFrame for Sunburst with columns ['id', 'parent', 'value'].\n    \"\"\"\n    df_all_trees = pd.DataFrame(columns=['id', 'parent', 'value'])\n    \n    for i, level in enumerate(levels):\n        df_tree = pd.DataFrame(columns=['id', 'parent', 'value'])\n        \n        # Groupby based on the chosen metric\n        if metric == \"count\":\n            dfg = df.groupby(levels[i:]).count()\n        else:\n            dfg = df.groupby(levels[i:]).sum()\n        \n        dfg = dfg.reset_index()\n        df_tree['id'] = dfg[level].copy()\n\n        # Set parent of the levels\n        if i &lt; len(levels) - 1:\n            df_tree['parent'] = dfg[levels[i+1]].copy()\n        else:\n            df_tree['parent'] = 'Total'\n        \n        df_tree['value'] = dfg[value_column]\n        df_all_trees = pd.concat([df_all_trees, df_tree], ignore_index=True)\n    \n    # Value calculation for the parent\n    if metric == \"count\":\n        total = pd.Series(dict(id='Total', parent='', value=df[value_column].count()))\n    else:\n        total = pd.Series(dict(id='Total', parent='', value=df[value_column].sum()))\n    \n    # Add frames one below the other to form the final dataframe\n    df_all_trees = pd.concat([df_all_trees, pd.DataFrame([total])], ignore_index=True)\n    return df_all_trees\n\n\nlevels = ['City', 'State', 'Country'] \nvalue_column = 'Population'\n\n\n(1) Hierarchical Sum dataframe\nThis dataframe represents total population accross Country, State and City under study.\n\ndf_sum=build_hierarchical_dataframe(data, levels, value_column, metric=\"sum\")\ndf_sum.head()\n\n\n\n\n\n\n\n\nid\nparent\nvalue\n\n\n\n\n0\nA1\nINMP\n512\n\n\n1\nB2\nINCG\n12201\n\n\n2\nC2\nUSNY\n812\n\n\n3\nD1\nINSD\n9104\n\n\n4\nE2\nINGD\n132\n\n\n\n\n\n\n\n\n\n(2) Hierarchical Count dataframe\nThis dataframe represents number of sub-classes (like City) accross Country and State under study.\n\ndf_count=build_hierarchical_dataframe(data, levels, value_column, metric=\"count\")\ndf_count.head()\n\n\n\n\n\n\n\n\nid\nparent\nvalue\n\n\n\n\n0\nA1\nINMP\n1\n\n\n1\nB2\nINCG\n1\n\n\n2\nC2\nUSNY\n1\n\n\n3\nD1\nINSD\n1\n\n\n4\nE2\nINGD\n1"
  },
  {
    "objectID": "posts/hierarchical/Plotly-Sunburst.html#visualisation",
    "href": "posts/hierarchical/Plotly-Sunburst.html#visualisation",
    "title": "Hierarchical Data Visualization Demystified",
    "section": "Visualisation",
    "text": "Visualisation\nNow we would see the two most common ways of plotting sunburst charts in python. The user can choose any of the following modules,\n\nPlotly Express\nPlotly Graph Objects\n\nBoth of these modules generate same “figure object”. Just the difference comes in code syntax and in flexibility of modifying graph as required. Plotly express is more of generating plot by calling function from already defined set of parameters. One may be more comfortable in tweaking the details while working with graph objects. However, the beauty of plotly is that you are able do the same things in the figure generated from plotly express as those are possible in that with graph objects. \nWe will be using both of them, and generate the plots for the datasets generated in the above section.\n\nfrom io import StringIO\nfrom IPython.display import display_html, HTML\n\n\n(1) Plotly Express\n\nimport plotly.express as px \n\nfigure = px.sunburst(data, path=['Country', 'State', 'City'], values='Population')\nfigure.update_layout(margin=dict(t=10, b=10, r=10, l=10))\nfigure.show() \n# HTML(figure.to_html(include_plotlyjs='cdn'))\n\n                                                \n\n\n\n\n(2) Graph Objects\n\nimport plotly.graph_objects as go\n\nfigure = go.Figure()\nfigure.add_trace(go.Sunburst(\n        labels=df_sum['id'],\n        parents=df_sum['parent'],\n        values=df_sum['value'],\n        branchvalues='total',\n        marker=dict(colorscale='Rdbu'),\n        hovertemplate='&lt;b&gt; Country : %{label} &lt;/b&gt; &lt;br&gt; Count : %{value} &lt;extra&gt;Population&lt;/extra&gt;',\n        maxdepth=2)\n    )\nfigure.update_layout(margin=dict(t=10, b=10, r=10, l=10))\nfigure.show() \n# HTML(figure.to_html(include_plotlyjs='cdn'))"
  },
  {
    "objectID": "posts/hierarchical/Plotly-Sunburst.html#communicating-plots-with-json",
    "href": "posts/hierarchical/Plotly-Sunburst.html#communicating-plots-with-json",
    "title": "Hierarchical Data Visualization Demystified",
    "section": "Communicating Plots with JSON",
    "text": "Communicating Plots with JSON\nWe can take these plots and convert them to JSONs. This comes handy when we need the plots to communicate from server part of a web application to client. Plotly has in-built function to save figure as json : write_json(). Following cells show how to write and regenerate the plots.\n\nfigure.write_json(\"../data/Sunburst_Chart.json\")\n\n\nimport json\n\nopened_file = open(\"../data/Sunburst_Chart.json\")\nopened_fig = json.load(opened_file)\n\nfig_ = go.Figure(\n    data = opened_fig['data'],\n    layout = opened_fig['layout']\n    )\nfig_.show()\n# HTML(fig_.to_html())"
  },
  {
    "objectID": "posts/hierarchical/Plotly-Sunburst.html#custom-plots",
    "href": "posts/hierarchical/Plotly-Sunburst.html#custom-plots",
    "title": "Hierarchical Data Visualization Demystified",
    "section": "Custom Plots",
    "text": "Custom Plots\nIn this final section we would see the go.Figure subplots, where fully customize the plots.\n\nfrom plotly.subplots import make_subplots\n\nfig = make_subplots(1, 2, specs=[[{\"type\": \"domain\"}, {\"type\": \"domain\"}]],)\nfig.add_trace(go.Sunburst(\n    labels=df_sum['id'],\n    parents=df_sum['parent'],\n    values=df_sum['value'],\n    branchvalues='total',\n    marker=dict(colorscale='sunset'),\n    hovertemplate='&lt;b&gt; Country : %{label} &lt;/b&gt; &lt;br&gt; Count : %{value} &lt;extra&gt;Population&lt;/extra&gt;',\n    maxdepth=2), 1, 1)\n\nfig.add_trace(go.Sunburst(\n    labels=df_count['id'],\n    parents=df_count['parent'],\n    values=df_count['value'],\n    branchvalues='total',\n    marker=dict(colorscale='viridis'),\n    hovertemplate='&lt;b&gt; Country : %{label} &lt;/b&gt; &lt;br&gt; Count : %{value} &lt;extra&gt;Cities&lt;/extra&gt;',\n    maxdepth=2), 1, 2)\n\nfig.update_layout(margin=dict(t=10, b=10, r=10, l=10))\nfig.show()\n# HTML(fig.to_html())"
  },
  {
    "objectID": "posts/hierarchical/Plotly-Sunburst.html#only-sunburst-what-are-some-alternatives-to-sunburst",
    "href": "posts/hierarchical/Plotly-Sunburst.html#only-sunburst-what-are-some-alternatives-to-sunburst",
    "title": "Hierarchical Data Visualization Demystified",
    "section": "Only Sunburst ? What are some alternatives to Sunburst ?",
    "text": "Only Sunburst ? What are some alternatives to Sunburst ?\nSunburst is one of the ways of visualizing the Hierarchical Data, we can also visualize such datasets using Treemap charts. For example -\n\nfig = px.treemap(data, \n                 path=[px.Constant(\"World\"), 'Country', 'State', 'City'], \n                 values='Population')\nfig.update_layout(margin = dict(t=50, l=25, r=25, b=25))\nfig.show()"
  },
  {
    "objectID": "posts/kaggle/loan-status-eda-classification.html",
    "href": "posts/kaggle/loan-status-eda-classification.html",
    "title": "Loan Status Prediction",
    "section": "",
    "text": "The goal of this project is to develop an automated system for predicting loan eligibility based on customer details provided through an online application form. The company aims to streamline and optimize their loan approval process by leveraging data to identify customer segments that are most likely to be eligible for a loan. This will enable targeted marketing and more efficient processing of applications."
  },
  {
    "objectID": "posts/kaggle/loan-status-eda-classification.html#importing-libraries",
    "href": "posts/kaggle/loan-status-eda-classification.html#importing-libraries",
    "title": "Loan Status Prediction",
    "section": "Importing Libraries",
    "text": "Importing Libraries\n\nimport  numpy as np\nimport  pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn.metrics import accuracy_score \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\nimport warnings\nwarnings.filterwarnings('ignore')"
  },
  {
    "objectID": "posts/kaggle/loan-status-eda-classification.html#importing-data",
    "href": "posts/kaggle/loan-status-eda-classification.html#importing-data",
    "title": "Loan Status Prediction",
    "section": "Importing data",
    "text": "Importing data\n\ntrain = pd.read_csv('../input/loan-prediction-problem-dataset/train_u6lujuX_CVtuZ9i.csv')\ntest = pd.read_csv('../input/loan-prediction-problem-dataset/test_Y3wMUE5_7gLdaTN.csv')\n\n\nprint (train.shape, test.shape)\n\n(614, 13) (367, 12)"
  },
  {
    "objectID": "posts/kaggle/loan-status-eda-classification.html#data-exploration",
    "href": "posts/kaggle/loan-status-eda-classification.html#data-exploration",
    "title": "Loan Status Prediction",
    "section": "Data Exploration",
    "text": "Data Exploration\n\ntrain.head() \n\n\n\n\n\n\n\n\nLoan_ID\nGender\nMarried\nDependents\nEducation\nSelf_Employed\nApplicantIncome\nCoapplicantIncome\nLoanAmount\nLoan_Amount_Term\nCredit_History\nProperty_Area\nLoan_Status\n\n\n\n\n0\nLP001002\nMale\nNo\n0\nGraduate\nNo\n5849\n0.0\nNaN\n360.0\n1.0\nUrban\nY\n\n\n1\nLP001003\nMale\nYes\n1\nGraduate\nNo\n4583\n1508.0\n128.0\n360.0\n1.0\nRural\nN\n\n\n2\nLP001005\nMale\nYes\n0\nGraduate\nYes\n3000\n0.0\n66.0\n360.0\n1.0\nUrban\nY\n\n\n3\nLP001006\nMale\nYes\n0\nNot Graduate\nNo\n2583\n2358.0\n120.0\n360.0\n1.0\nUrban\nY\n\n\n4\nLP001008\nMale\nNo\n0\nGraduate\nNo\n6000\n0.0\n141.0\n360.0\n1.0\nUrban\nY\n\n\n\n\n\n\n\n\ntrain.info() \n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 614 entries, 0 to 613\nData columns (total 13 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   Loan_ID            614 non-null    object \n 1   Gender             601 non-null    object \n 2   Married            611 non-null    object \n 3   Dependents         599 non-null    object \n 4   Education          614 non-null    object \n 5   Self_Employed      582 non-null    object \n 6   ApplicantIncome    614 non-null    int64  \n 7   CoapplicantIncome  614 non-null    float64\n 8   LoanAmount         592 non-null    float64\n 9   Loan_Amount_Term   600 non-null    float64\n 10  Credit_History     564 non-null    float64\n 11  Property_Area      614 non-null    object \n 12  Loan_Status        614 non-null    object \ndtypes: float64(4), int64(1), object(8)\nmemory usage: 62.5+ KB\n\n\n\ntrain.isnull().sum()\n\nLoan_ID               0\nGender               13\nMarried               3\nDependents           15\nEducation             0\nSelf_Employed        32\nApplicantIncome       0\nCoapplicantIncome     0\nLoanAmount           22\nLoan_Amount_Term     14\nCredit_History       50\nProperty_Area         0\nLoan_Status           0\ndtype: int64\n\n\n\ntest.head()\n\n\n\n\n\n\n\n\nLoan_ID\nGender\nMarried\nDependents\nEducation\nSelf_Employed\nApplicantIncome\nCoapplicantIncome\nLoanAmount\nLoan_Amount_Term\nCredit_History\nProperty_Area\n\n\n\n\n0\nLP001015\nMale\nYes\n0\nGraduate\nNo\n5720\n0\n110.0\n360.0\n1.0\nUrban\n\n\n1\nLP001022\nMale\nYes\n1\nGraduate\nNo\n3076\n1500\n126.0\n360.0\n1.0\nUrban\n\n\n2\nLP001031\nMale\nYes\n2\nGraduate\nNo\n5000\n1800\n208.0\n360.0\n1.0\nUrban\n\n\n3\nLP001035\nMale\nYes\n2\nGraduate\nNo\n2340\n2546\n100.0\n360.0\nNaN\nUrban\n\n\n4\nLP001051\nMale\nNo\n0\nNot Graduate\nNo\n3276\n0\n78.0\n360.0\n1.0\nUrban\n\n\n\n\n\n\n\n\ntest.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 367 entries, 0 to 366\nData columns (total 12 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   Loan_ID            367 non-null    object \n 1   Gender             356 non-null    object \n 2   Married            367 non-null    object \n 3   Dependents         357 non-null    object \n 4   Education          367 non-null    object \n 5   Self_Employed      344 non-null    object \n 6   ApplicantIncome    367 non-null    int64  \n 7   CoapplicantIncome  367 non-null    int64  \n 8   LoanAmount         362 non-null    float64\n 9   Loan_Amount_Term   361 non-null    float64\n 10  Credit_History     338 non-null    float64\n 11  Property_Area      367 non-null    object \ndtypes: float64(3), int64(2), object(7)\nmemory usage: 34.5+ KB\n\n\n\ntest.isnull().sum()\n\nLoan_ID               0\nGender               11\nMarried               0\nDependents           10\nEducation             0\nSelf_Employed        23\nApplicantIncome       0\nCoapplicantIncome     0\nLoanAmount            5\nLoan_Amount_Term      6\nCredit_History       29\nProperty_Area         0\ndtype: int64"
  },
  {
    "objectID": "posts/kaggle/loan-status-eda-classification.html#data-preparation-processing",
    "href": "posts/kaggle/loan-status-eda-classification.html#data-preparation-processing",
    "title": "Loan Status Prediction",
    "section": "Data Preparation / Processing",
    "text": "Data Preparation / Processing\n\ndata = [train,test]\nfor dataset in data:\n    #Filter categorical variables\n    categorical_columns = [x for x in dataset.dtypes.index if dataset.dtypes[x]=='object']\n    # Exclude ID cols and source:\n    categorical_columns = [x for x in categorical_columns if x not in ['Loan_ID' ]]\n    #Print frequency of categories\n    \nfor col in categorical_columns:\n    print ('\\nFrequency of Categories for variable %s'%col)\n    print (train[col].value_counts())\n\n\nFrequency of Categories for variable Gender\nGender\nMale      489\nFemale    112\nName: count, dtype: int64\n\nFrequency of Categories for variable Married\nMarried\nYes    398\nNo     213\nName: count, dtype: int64\n\nFrequency of Categories for variable Dependents\nDependents\n0     345\n1     102\n2     101\n3+     51\nName: count, dtype: int64\n\nFrequency of Categories for variable Education\nEducation\nGraduate        480\nNot Graduate    134\nName: count, dtype: int64\n\nFrequency of Categories for variable Self_Employed\nSelf_Employed\nNo     500\nYes     82\nName: count, dtype: int64\n\nFrequency of Categories for variable Property_Area\nProperty_Area\nSemiurban    233\nUrban        202\nRural        179\nName: count, dtype: int64\n\n\n\nGender\n\nsns.countplot(train['Gender'])\n\n\n\n\n\n\n\n\n\npd.crosstab(train.Gender, train.Loan_Status, margins = True)\n\n\n\n\n\n\n\nLoan_Status\nN\nY\nAll\n\n\nGender\n\n\n\n\n\n\n\nFemale\n37\n75\n112\n\n\nMale\n150\n339\n489\n\n\nAll\n187\n414\n601\n\n\n\n\n\n\n\nThe male are in large number as compared to female applicants.Also many of them have positive Loan Status. Further Binarization of this feature should be done,\n\ntrain.Gender = train.Gender.fillna(train.Gender.mode())\ntest.Gender = test.Gender.fillna(test.Gender.mode())\n\nsex = pd.get_dummies(train['Gender'] , drop_first = True )\ntrain.drop(['Gender'], axis = 1 , inplace =True)\ntrain = pd.concat([train , sex ] , axis = 1)\n\nsex = pd.get_dummies(test['Gender'] , drop_first = True )\ntest.drop(['Gender'], axis = 1 , inplace =True)\ntest = pd.concat([test , sex ] , axis = 1)\n\n\n\nDependants\n\nplt.figure(figsize=(6,6))\nlabels = ['0' , '1', '2' , '3+']\nexplode = (0.05, 0, 0, 0)\nsize = [345 , 102 , 101 , 51]\n\nplt.pie(size, explode=explode, labels=labels,\n        autopct='%1.1f%%', shadow = True, startangle = 90)\nplt.axis('equal')\nplt.show()\n\n\n\n\n\n\n\n\n\ntrain.Dependents.value_counts()\n\nDependents\n0     345\n1     102\n2     101\n3+     51\nName: count, dtype: int64\n\n\n\npd.crosstab(train.Dependents , train.Loan_Status, margins = True)\n\n\n\n\n\n\n\nLoan_Status\nN\nY\nAll\n\n\nDependents\n\n\n\n\n\n\n\n0\n107\n238\n345\n\n\n1\n36\n66\n102\n\n\n2\n25\n76\n101\n\n\n3+\n18\n33\n51\n\n\nAll\n186\n413\n599\n\n\n\n\n\n\n\nThe applicants with highest number of dependants are least in number whereas applicants with no dependance are greatest among these.\n\ntrain.Dependents = train.Dependents.fillna(\"0\")\ntest.Dependents = test.Dependents.fillna(\"0\")\n\nrpl = {'0':'0', '1':'1', '2':'2', '3+':'3'}\n\ntrain.Dependents = train.Dependents.replace(rpl).astype(int)\ntest.Dependents = test.Dependents.replace(rpl).astype(int)\n\n\n\nCredit History\n\npd.crosstab(train.Credit_History , train.Loan_Status, margins = True)\n\n\n\n\n\n\n\nLoan_Status\nN\nY\nAll\n\n\nCredit_History\n\n\n\n\n\n\n\n0.0\n82\n7\n89\n\n\n1.0\n97\n378\n475\n\n\nAll\n179\n385\n564\n\n\n\n\n\n\n\n\ntrain.Credit_History = train.Credit_History.fillna(train.Credit_History.mode()[0])\ntest.Credit_History  = test.Credit_History.fillna(test.Credit_History.mode()[0])\n\n\nSelf Employed\n\nsns.countplot(train['Self_Employed'])\n\n\n\n\n\n\n\n\n\npd.crosstab(train.Self_Employed , train.Loan_Status,margins = True)\n\n\n\n\n\n\n\nLoan_Status\nN\nY\nAll\n\n\nSelf_Employed\n\n\n\n\n\n\n\nNo\n157\n343\n500\n\n\nYes\n26\n56\n82\n\n\nAll\n183\n399\n582\n\n\n\n\n\n\n\n\ntrain.Self_Employed = train.Self_Employed.fillna(train.Self_Employed.mode())\ntest.Self_Employed = test.Self_Employed.fillna(test.Self_Employed.mode())\n\nself_Employed = pd.get_dummies(train['Self_Employed'] ,prefix = 'employed' ,drop_first = True )\ntrain.drop(['Self_Employed'], axis = 1 , inplace =True)\ntrain = pd.concat([train , self_Employed ] , axis = 1)\n\nself_Employed = pd.get_dummies(test['Self_Employed'] , prefix = 'employed' ,drop_first = True )\ntest.drop(['Self_Employed'], axis = 1 , inplace =True)\ntest = pd.concat([test , self_Employed ] , axis = 1)\n\n\n\n\nMarried\n\nsns.countplot(train.Married)\n\n\n\n\n\n\n\n\n\npd.crosstab(train.Married , train.Loan_Status,margins = True)\n\n\n\n\n\n\n\nLoan_Status\nN\nY\nAll\n\n\nMarried\n\n\n\n\n\n\n\nNo\n79\n134\n213\n\n\nYes\n113\n285\n398\n\n\nAll\n192\n419\n611\n\n\n\n\n\n\n\n\ntrain.Married = train.Married.fillna(train.Married.mode())\ntest.Married = test.Married.fillna(test.Married.mode())\n\nmarried = pd.get_dummies(train['Married'] , prefix = 'married',drop_first = True )\ntrain.drop(['Married'], axis = 1 , inplace =True)\ntrain = pd.concat([train , married ] , axis = 1)\n\nmarried = pd.get_dummies(test['Married'] , prefix = 'married', drop_first = True )\ntest.drop(['Married'], axis = 1 , inplace =True)\ntest = pd.concat([test , married ] , axis = 1)\n\n\n\nLoan Amount Term and Loan Amount\n\ntrain.drop(['Loan_Amount_Term'], axis = 1 , inplace =True)\ntest.drop(['Loan_Amount_Term'], axis = 1 , inplace =True)\n\ntrain.LoanAmount = train.LoanAmount.fillna(train.LoanAmount.mean()).astype(int)\ntest.LoanAmount = test.LoanAmount.fillna(test.LoanAmount.mean()).astype(int)\n\n\nsns.distplot(train['LoanAmount'])\n\n\n\n\n\n\n\n\nWe observe no outliers in the continuous variable Loan Amount\n\n\nEducation\n\nsns.countplot(train.Education)\n\n\n\n\n\n\n\n\n\ntrain['Education'] = train['Education'].map( {'Graduate': 0, 'Not Graduate': 1} ).astype(int)\ntest['Education'] = test['Education'].map( {'Graduate': 0, 'Not Graduate': 1} ).astype(int)\n\n\n\nProperty Area\n\nsns.countplot(train.Property_Area)\n\n\n\n\n\n\n\n\n\ntrain['Property_Area'] = train['Property_Area'].map( {'Urban': 0, 'Semiurban': 1 ,'Rural': 2  } ).astype(int)\n\ntest.Property_Area = test.Property_Area.fillna(test.Property_Area.mode())\ntest['Property_Area'] = test['Property_Area'].map( {'Urban': 0, 'Semiurban': 1 ,'Rural': 2  } ).astype(int)\n\n\n\nCo-Applicant income and Applicant income\n\nsns.distplot(train['ApplicantIncome'])\n\n\n\n\n\n\n\n\n\nsns.distplot(train['CoapplicantIncome'])\n\n\n\n\n\n\n\n\n\n\nTarget Variable : Loan Status\n\ntrain['Loan_Status'] = train['Loan_Status'].map( {'N': 0, 'Y': 1 } ).astype(int)\n\n\nDropping the ID column\n\ntrain.drop(['Loan_ID'], axis = 1 , inplace =True)"
  },
  {
    "objectID": "posts/kaggle/loan-status-eda-classification.html#view-the-datasets",
    "href": "posts/kaggle/loan-status-eda-classification.html#view-the-datasets",
    "title": "Loan Status Prediction",
    "section": "View the datasets",
    "text": "View the datasets\n\ntrain.head()\n\n\n\n\n\n\n\n\nDependents\nEducation\nApplicantIncome\nCoapplicantIncome\nLoanAmount\nCredit_History\nProperty_Area\nLoan_Status\nMale\nemployed_Yes\nmarried_Yes\n\n\n\n\n0\n0\n0\n5849\n0.0\n146\n1.0\n0\n1\nTrue\nFalse\nFalse\n\n\n1\n1\n0\n4583\n1508.0\n128\n1.0\n2\n0\nTrue\nFalse\nTrue\n\n\n2\n0\n0\n3000\n0.0\n66\n1.0\n0\n1\nTrue\nTrue\nTrue\n\n\n3\n0\n1\n2583\n2358.0\n120\n1.0\n0\n1\nTrue\nFalse\nTrue\n\n\n4\n0\n0\n6000\n0.0\n141\n1.0\n0\n1\nTrue\nFalse\nFalse\n\n\n\n\n\n\n\n\ntest.head()\n\n\n\n\n\n\n\n\nLoan_ID\nDependents\nEducation\nApplicantIncome\nCoapplicantIncome\nLoanAmount\nCredit_History\nProperty_Area\nMale\nemployed_Yes\nmarried_Yes\n\n\n\n\n0\nLP001015\n0\n0\n5720\n0\n110\n1.0\n0\nTrue\nFalse\nTrue\n\n\n1\nLP001022\n1\n0\n3076\n1500\n126\n1.0\n0\nTrue\nFalse\nTrue\n\n\n2\nLP001031\n2\n0\n5000\n1800\n208\n1.0\n0\nTrue\nFalse\nTrue\n\n\n3\nLP001035\n2\n0\n2340\n2546\n100\n1.0\n0\nTrue\nFalse\nTrue\n\n\n4\nLP001051\n0\n1\n3276\n0\n78\n1.0\n0\nTrue\nFalse\nFalse"
  },
  {
    "objectID": "posts/kaggle/loan-status-eda-classification.html#boxplots-for-relation-between-property-area-amount-of-loan-and-education-qualification",
    "href": "posts/kaggle/loan-status-eda-classification.html#boxplots-for-relation-between-property-area-amount-of-loan-and-education-qualification",
    "title": "Loan Status Prediction",
    "section": "Boxplots for relation between Property area, amount of Loan and Education qualification",
    "text": "Boxplots for relation between Property area, amount of Loan and Education qualification\nFurther we analyse the relation between education status,loan taken and property area\n\nProperty_Area:\n\nUrban      :0\nSemiurban  :1\nRural      :2\n\n\n\nplt.figure(figsize=(5,2))\nsns.boxplot(x=\"Property_Area\", y=\"LoanAmount\", hue=\"Education\",data=train, palette=\"coolwarm\")\n\n\n\n\n\n\n\n\n\nThe above boxplot signifies that,\n\nIn the Urban area the non graduates take slightly more loan than graduates.\nIn the Rural and semiurban area the graduates take more amount of Loan than non graduates\nThe higher values of Loan are mostly from Urban area\nThe semiurban area and rural area both have one unusual Loan amount close to zero."
  },
  {
    "objectID": "posts/kaggle/loan-status-eda-classification.html#crosstab-for-relation-between-credit-history-and-loan-status.",
    "href": "posts/kaggle/loan-status-eda-classification.html#crosstab-for-relation-between-credit-history-and-loan-status.",
    "title": "Loan Status Prediction",
    "section": "Crosstab for relation between Credit History and Loan status.",
    "text": "Crosstab for relation between Credit History and Loan status.\n\ntrain.Credit_History.value_counts()\n\nCredit_History\n1.0    525\n0.0     89\nName: count, dtype: int64\n\n\n\nlc = pd.crosstab(train['Credit_History'], train['Loan_Status'])\nlc.plot(kind='bar', stacked=True, color=['red','blue'], grid=False)\n\n\n\n\n\n\n\n\n\nThe credit history vs Loan Status indicates:\n\nThe good credit history applicants have more chances of getting Loan.\nWith better credit History the Loan amount given was greater too.\nBut many were not given loan in the range 0-100\nThe applicant with poor credit history were handled in the range 0-100 only.\n\n\n\nplt.figure(figsize=(9,6))\nsns.heatmap(train.drop('Loan_Status',axis=1).corr(), vmax=0.6, square=True, annot=True)"
  },
  {
    "objectID": "posts/kaggle/loan-status-eda-classification.html#prediction",
    "href": "posts/kaggle/loan-status-eda-classification.html#prediction",
    "title": "Loan Status Prediction",
    "section": "Prediction",
    "text": "Prediction\nThe problem is of Classification as observed and concluded from the data and visualisations.\n\nX = train.drop('Loan_Status' , axis = 1 )\ny = train['Loan_Status']\n\nX_train ,X_test , y_train , y_test = train_test_split(X , y , test_size = 0.3 , random_state =102)\n\n\nfrom sklearn.linear_model import LogisticRegression\nlogmodel = LogisticRegression()\nlogmodel.fit(X_train , y_train)\npred_l = logmodel.predict(X_test)\nacc_l = accuracy_score(y_test , pred_l)*100\nacc_l\n\n83.78378378378379\n\n\n\nrandom_forest = RandomForestClassifier(n_estimators= 100)\nrandom_forest.fit(X_train, y_train)\npred_rf = random_forest.predict(X_test)\nacc_rf = accuracy_score(y_test , pred_rf)*100\nacc_rf\n\n80.54054054054053\n\n\n\nknn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, y_train)\npred_knn = knn.predict(X_test)\nacc_knn = accuracy_score(y_test , pred_knn)*100\nacc_knn\n\n61.08108108108108\n\n\n\ngaussian = GaussianNB()\ngaussian.fit(X_train, y_train)\npred_gb = gaussian.predict(X_test)\nacc_gb = accuracy_score(y_test , pred_gb)*100\nacc_gb\n\n82.16216216216216\n\n\n\nsvc = SVC()\nsvc.fit(X_train, y_train)\npred_svm = svc.predict(X_test)\nacc_svm = accuracy_score(y_test , pred_svm)*100\nacc_svm\n\n70.27027027027027\n\n\n\ngbk = GradientBoostingClassifier()\ngbk.fit(X_train, y_train)\npred_gbc = gbk.predict(X_test)\nacc_gbc = accuracy_score(y_test , pred_gbc)*100\nacc_gbc\n\n82.16216216216216\n\n\n\n## Arranging the Accuracy results\nmodels = pd.DataFrame({\n    'Model': ['Logistic Regression', 'Random Forrest','K- Nearest Neighbour' ,\n             'Naive Bayes' , 'SVM','Gradient Boosting Classifier'],\n    'Score': [acc_l , acc_rf , acc_knn , acc_gb ,acc_svm ,acc_gbc ]})\nmodels.sort_values(by='Score', ascending=False)\n\n\n\n\n\n\n\n\nModel\nScore\n\n\n\n\n0\nLogistic Regression\n83.783784\n\n\n3\nNaive Bayes\n82.162162\n\n\n5\nGradient Boosting Classifier\n82.162162\n\n\n1\nRandom Forrest\n80.540541\n\n\n4\nSVM\n70.270270\n\n\n2\nK- Nearest Neighbour\n61.081081\n\n\n\n\n\n\n\nThe highest classification accuracy is shown by Logistic Regression of about 83.24 %\nLet us Check th feature importance,\n\nimportances = pd.DataFrame({'Features':X_train.columns,'Importance':np.round(random_forest.feature_importances_,3)})\nimportances = importances.sort_values('Importance',ascending=False).set_index('Features')\nimportances.head(11) \n\n\n\n\n\n\n\n\nImportance\n\n\nFeatures\n\n\n\n\n\nCredit_History\n0.248\n\n\nApplicantIncome\n0.216\n\n\nLoanAmount\n0.211\n\n\nCoapplicantIncome\n0.122\n\n\nDependents\n0.053\n\n\nProperty_Area\n0.052\n\n\nEducation\n0.027\n\n\nmarried_Yes\n0.026\n\n\nMale\n0.024\n\n\nemployed_Yes\n0.021\n\n\n\n\n\n\n\n\nimportances.plot.bar()\n\n\n\n\n\n\n\n\nCredit History has the maximum importance and empoloyment has the least!\n\nSummarizing\nThe Loan status has better relation with features such as Credit History, Applicant’s Income, Loan Amount needed by them, Family status(Depenedents) and Property Area which are generally considered by the loan providing organisations. These factors are hence used to take correct decisions to provide loan status or not. This data analysis hence gives a realisation of features and the relation between them from the older decision examples hence giving a learning to predict the class of the unseen data.\nFinally the we predict over unseen dataset using the Logistic Regression and Random Forest model(Ensemble Learning):\n\ndf_test = test.drop(['Loan_ID'], axis = 1)\n\n\ndf_test.head()\n\n\n\n\n\n\n\n\nDependents\nEducation\nApplicantIncome\nCoapplicantIncome\nLoanAmount\nCredit_History\nProperty_Area\nMale\nemployed_Yes\nmarried_Yes\n\n\n\n\n0\n0\n0\n5720\n0\n110\n1.0\n0\nTrue\nFalse\nTrue\n\n\n1\n1\n0\n3076\n1500\n126\n1.0\n0\nTrue\nFalse\nTrue\n\n\n2\n2\n0\n5000\n1800\n208\n1.0\n0\nTrue\nFalse\nTrue\n\n\n3\n2\n0\n2340\n2546\n100\n1.0\n0\nTrue\nFalse\nTrue\n\n\n4\n0\n1\n3276\n0\n78\n1.0\n0\nTrue\nFalse\nFalse\n\n\n\n\n\n\n\n\np_log = logmodel.predict(df_test)\n\n\np_rf = random_forest.predict(df_test)\n\n\npredict_combine = np.zeros((df_test.shape[0]))\n\nfor i in range(0, test.shape[0]):\n    temp = p_log[i] + p_rf[i]\n    if temp&gt;=2:\n        predict_combine[i] = 1\npredict_combine = predict_combine.astype('int')\n\n\nsubmission = pd.DataFrame({\n        \"Loan_ID\": test[\"Loan_ID\"],\n        \"Loan_Status\": predict_combine\n    })\n\nsubmission.to_csv(\"results.csv\", encoding='utf-8', index=False)"
  }
]
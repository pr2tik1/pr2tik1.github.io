[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": " devExplore ",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nSimplifying R Package Management - renv vs packrat\n\n\n\n\n\n\nR\n\n\nCoding Practices\n\n\n\n\n\n\n\n\n\nJan 19, 2024\n\n\nPratik Kumar\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nEfficient project management using Dockers\n\n\n\n\n\n\nDockers\n\n\nPython\n\n\nR\n\n\nCoding Practices\n\n\n\n\n\n\n\n\n\nJul 31, 2023\n\n\nPratik Kumar\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nHierarchical Data Visualization Demystified\n\n\n\n\n\n\nData Visualization\n\n\nPlotly\n\n\nPython\n\n\nCoding Practices\n\n\n\n\n\n\n\n\n\nJun 19, 2023\n\n\nPratik Kumar\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\nLoan Status Prediction\n\n\n\n\n\n\nData Science\n\n\nData Visualization\n\n\nPython\n\n\nMachine Learning\n\n\nFeature Engineering\n\n\nKaggle\n\n\n\n\n\n\n\n\n\nApr 25, 2021\n\n\nPratik Kumar\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\nTitanic Survival Prediction\n\n\n\n\n\n\nData Science\n\n\nData Visualization\n\n\nPython\n\n\nMachine Learning\n\n\nFeature Engineering\n\n\nKaggle\n\n\n\n\n\n\n\n\n\nMar 21, 2021\n\n\nPratik Kumar\n\n\n15 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/remote-extension/remote-containers.html",
    "href": "posts/remote-extension/remote-containers.html",
    "title": "Efficient project management using Dockers",
    "section": "",
    "text": "Dockers are excellent tools in software development. The key benefit of Dockers is that they allow users to package an application with all its dependencies into a standardized unit for software development. They have a low impact on the operating system and are very fast, using minimal disk space. Instead of encapsulating the entire machine, they encapsulate the environments for an application, making it easy to share code, rebuild applications, and distribute them.\nUsing Microsoft’s extension called : Dev Containers we can harness these benefits. The Dev Containers extension enables the use of a Docker container as a full-featured development environment. This helps developers in the following ways:\n\nDevelop with a consistent and reproducible environment.\nQuickly swap between different, separate development environments and safely make updates without worrying about impacting your local machine.\nMake it easy for new team members / contributors to get up and running in a consistent development environment.\n\nPersonally, I have been using Dev Containers to manage projects with different requirements. As a software engineer at Elucidata, I am responsible for developing various applications that require different versions of R, Python, and JavaScript. Remote containers have been immensely helpful in keeping the dedicated packages for each application within their project directories only.\nWhile there are other alternatives such as virtual environments (e.g.- pyenv, venv) and package management tools (for e.g. - packrat, renv), containerization proves to be more efficient for bigger projects where we have to manage these R and Python environments together. With minimal code, we can instruct the required installations and quickly reproduce the environment.\nThe Dev Containers extension supports two primary operating models:\n\nYou can use a container as your full-time development environment\nYou can attach to a running container to inspect it."
  },
  {
    "objectID": "posts/remote-extension/remote-containers.html#requirements",
    "href": "posts/remote-extension/remote-containers.html#requirements",
    "title": "Efficient project management using Dockers",
    "section": "Requirements",
    "text": "Requirements\nYou can use Docker with the Dev Containers extension in a few ways, including:\n\nDocker installed locally\nDocker installed on a remote environment\nOther Docker compliant CLIs, installed locally or remotely\n\nWhile other CLIs may work, they are not officially supported. Note that attaching to a Kubernetes cluster only requires a properly configured kubectl CLI\n\n\nVisit here to know more about system requirements."
  },
  {
    "objectID": "posts/remote-extension/remote-containers.html#installation",
    "href": "posts/remote-extension/remote-containers.html#installation",
    "title": "Efficient project management using Dockers",
    "section": "Installation",
    "text": "Installation\nTo get started, follow this official step by step tutorial here."
  },
  {
    "objectID": "posts/remote-extension/remote-containers.html#devcontainerdevcontainer.json",
    "href": "posts/remote-extension/remote-containers.html#devcontainerdevcontainer.json",
    "title": "Efficient project management using Dockers",
    "section": ".devcontainer/devcontainer.json",
    "text": ".devcontainer/devcontainer.json\nWithin the .devcontainer folder the devcontainer.json config file helps the extension to determine the name of the container, which image to use, extensions to install, port to expose and other configurations. Few examples,\n\n1. Shiny Application\nConsider there are multiple projects that you are working on, and each of them require different R versions and packages. You can set the value of the image key from below to any version of choice for R.\n{\n    \"name\" : \"Project 1\",\n    \"image\": \"r-base:latest\" //Any version as per requirement from dockerhub\n}\n{\n    \"name\" : \"Project 2\",\n    \"image\": \"r-base:4.1\" //Any version as per requirement from dockerhub\n}\n\n\n2. Using Custom Docker Image\nNow, suppose you want to use your own image with a specific R version, package installations, and other tools/languages included. In that case, you can create a custom Dockerfile (just make sure to specify the correct path for this file) as follows,\n# Use the specified R base image\nFROM r-base:latest\n\n# Set non-interactive mode for apt-get\nENV DEBIAN_FRONTEND=noninteractive\n\n# Expose port 4200\nEXPOSE 4200\n\n# Install necessary system dependencies\nRUN apt-get update && apt-get install -y \\\n    make \\\n    build-essential \\\n    git \\\n    wget \\\n    curl \n\n## Python setup\n# Switch to user \"docker\" to install pyenv\nUSER docker\nRUN curl https://pyenv.run | bash\n\n# Set up Python paths\nENV PATH=\"/home/docker/.pyenv/bin:$PATH\"\nENV PATH=\"/home/docker/.pyenv/versions/3.9.13/bin:$PATH\"\n\n# Install Python 3.9.13\nRUN PYTHON_CONFIGURE_OPTS=\"--enable-shared\" pyenv install 3.9.13\n\n# Switch back to root user for the remaining steps\nUSER root\n\n## Python virtual env and packages\n# Copy requirements.txt into the image\nCOPY requirements.txt requirements.txt\n\n# Create and activate Python virtual environment\nRUN rm -rf my_venv && python3 -m venv my_venv && \\\n    . my_venv/bin/activate && \\\n    python3 -m pip install --upgrade pip && \\\n    pip install -r requirements.txt\n\n# Install R packages using renv\nCOPY renv.lock renv.lock\nRUN R -e \"install.packages('renv'); renv::restore()\"\nNow with this Dockerfile, run the extension using following devcontainer.json :\n{\n    \"name\": \"Custom App\",\n    \"build\": {\n      \"dockerfile\": \"Dockerfile\",  //path to your custom Dockerfile.\n      \"context\": \"..\"\n    },\n    \"remoteUser\": \"docker\"\n}\n\n\n3. Post Installation Step\nWhen it comes to development, there could be a constant update in list of packages in your project. Now building everytime for each newly added package can be time consuming. Hence, we can install packages after building the image with minimal layers. So that the files would like :\n\nThe updated config file .devcontainer/devcontainer.json would now look like:\n\n{\n    \"name\": \"Custom App\",\n    \"build\": {\n      \"dockerfile\": \"Dockerfile\", //path to your custom Dockerfile.\n      \"context\": \"..\"\n    },\n    \"remoteUser\": \"docker\",\n    \"postCreateCommand\": \"bash .devcontainer/build_environment.sh\" // Can use any scripting language of choice.\n}\n\nYou can use a bash script to install packages after the build of container .devcontainer/build_environment.sh :\n\n## Install R packages\nR -e \"install.packages('renv'); renv::restore()\"\n\n\n## Install python packages\nrm -rf my_venv && python3 -m venv my_venv && \\\n    . my_venv/bin/activate && \\\n    python3 -m pip install --upgrade pip && \\\n    pip install -r requirements.txt"
  },
  {
    "objectID": "posts/kaggle/titanic-survival-eda-feature-engineering-preds.html",
    "href": "posts/kaggle/titanic-survival-eda-feature-engineering-preds.html",
    "title": "Titanic Survival Prediction",
    "section": "",
    "text": "Welcome to the Titanic Survival Prediction project, a classic challenge that serves as the perfect starting point for anyone looking to dive into the world of Machine Learning and data competitions on Kaggle. This project involves building a predictive model to determine which passengers survived the infamous Titanic disaster, a task that will guide you through essential steps in data science, from feature engineering to model development and evaluation.\nThis competition is designed to help you get comfortable with the Kaggle platform and machine learning workflows. You’ll be using Python to explore the data, perform feature engineering, visualize key trends, and develop a predictive model that can accurately classify survivors. For more details, visit the competition page and check out Kaggle’s YouTube video for a comprehensive introduction.\n\nImport Data: Load the Titanic dataset to begin the exploration and analysis.\nFeature Engineering: Transform raw data into meaningful features that improve model performance.\nData Visualization: Analyze and visualize the data to uncover patterns and insights.\nModel Development: Build and train machine learning models to predict passenger survival.\nModel Testing: Evaluate model accuracy and fine-tune parameters to optimize results.\nPrediction and Submission: Generate survival predictions and submit them to the Kaggle leaderboard.\n\nEmbark on this journey to not only enhance your data science skills but also understand the power of predictive modeling in real-world scenarios."
  },
  {
    "objectID": "posts/kaggle/titanic-survival-eda-feature-engineering-preds.html#introduction",
    "href": "posts/kaggle/titanic-survival-eda-feature-engineering-preds.html#introduction",
    "title": "Titanic Survival Prediction",
    "section": "",
    "text": "Welcome to the Titanic Survival Prediction project, a classic challenge that serves as the perfect starting point for anyone looking to dive into the world of Machine Learning and data competitions on Kaggle. This project involves building a predictive model to determine which passengers survived the infamous Titanic disaster, a task that will guide you through essential steps in data science, from feature engineering to model development and evaluation.\nThis competition is designed to help you get comfortable with the Kaggle platform and machine learning workflows. You’ll be using Python to explore the data, perform feature engineering, visualize key trends, and develop a predictive model that can accurately classify survivors. For more details, visit the competition page and check out Kaggle’s YouTube video for a comprehensive introduction.\n\nImport Data: Load the Titanic dataset to begin the exploration and analysis.\nFeature Engineering: Transform raw data into meaningful features that improve model performance.\nData Visualization: Analyze and visualize the data to uncover patterns and insights.\nModel Development: Build and train machine learning models to predict passenger survival.\nModel Testing: Evaluate model accuracy and fine-tune parameters to optimize results.\nPrediction and Submission: Generate survival predictions and submit them to the Kaggle leaderboard.\n\nEmbark on this journey to not only enhance your data science skills but also understand the power of predictive modeling in real-world scenarios."
  },
  {
    "objectID": "posts/kaggle/titanic-survival-eda-feature-engineering-preds.html#a.-import-data",
    "href": "posts/kaggle/titanic-survival-eda-feature-engineering-preds.html#a.-import-data",
    "title": "Titanic Survival Prediction",
    "section": "A. Import Data",
    "text": "A. Import Data\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\ntrain = pd.read_csv('../input/titanic/train.csv')\ntest = pd.read_csv('../input/titanic/test.csv')"
  },
  {
    "objectID": "posts/kaggle/titanic-survival-eda-feature-engineering-preds.html#b.-dataset-exploration",
    "href": "posts/kaggle/titanic-survival-eda-feature-engineering-preds.html#b.-dataset-exploration",
    "title": "Titanic Survival Prediction",
    "section": "B. Dataset exploration:",
    "text": "B. Dataset exploration:\n\n\ntrain.head()\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\n\n\n\n\nB.1. Types of Variables\n\n# Find categorical variables\ncategorical = [var for var in train.columns if train[var].dtype=='O']\nprint('There are {} categorical variables'.format(len(categorical)))\n\nThere are 5 categorical variables\n\n\n\n# Find numerical variables\nnumerical = [var for var in train.columns if train[var].dtype!='O']\nprint('There are {} numerical variables'.format(len(numerical)))\n\nThere are 7 numerical variables\n\n\n\nViewing the Categorical terms :\n\ndata = [train,test]\nfor dataset in data:\n    #Filter categorical variables\n    categorical_columns = [x for x in dataset.dtypes.index if dataset.dtypes[x]=='object']    \n    # Exclude ID cols and source:\n    categorical_columns = [x for x in categorical_columns if x not in ['PassengerId','Ticket','Name','Cabin']]\n    #Print frequency of categories\n    \nfor col in categorical_columns:\n    print ('\\nFrequency of Categories for variable %s'%col)\n    print (train[col].value_counts())\n\n\nFrequency of Categories for variable Sex\nSex\nmale      577\nfemale    314\nName: count, dtype: int64\n\nFrequency of Categories for variable Embarked\nEmbarked\nS    644\nC    168\nQ     77\nName: count, dtype: int64\n\n\n\n\n\nB.2. Detecting Missing Values\n\ntrain.isnull().sum()\n\nPassengerId      0\nSurvived         0\nPclass           0\nName             0\nSex              0\nAge            177\nSibSp            0\nParch            0\nTicket           0\nFare             0\nCabin          687\nEmbarked         2\ndtype: int64\n\n\n\ntrain.isnull().mean()\n\nPassengerId    0.000000\nSurvived       0.000000\nPclass         0.000000\nName           0.000000\nSex            0.000000\nAge            0.198653\nSibSp          0.000000\nParch          0.000000\nTicket         0.000000\nFare           0.000000\nCabin          0.771044\nEmbarked       0.002245\ndtype: float64\n\n\n\n\nMissing Data Overview\nThe train dataset has 12 features, with missing values observed in the following features:\n\nAge: Missing in 19.86% of the records\nCabin: Missing in 77.10% of the records\nEmbarked: Missing in 0.22% of the records\n\n\n\nAnalysis and Assumptions About Missing Data\n\nCabin\nThe Cabin feature has the highest proportion of missing values (77.10%). This substantial amount of missing data might suggest that:\n\nFor many individuals who did not survive, the cabin information was not recorded or available.\nSurvivors, on the other hand, may have been able to provide this information.\n\nThe missingness here could be due to the nature of the records or circumstances surrounding the individuals who did not survive, making this data likely to fall into the Missing Not At Random (MNAR) category. This means the missingness is related to the unobserved value itself or other factors not accounted for.\n\n\nAge\nThe Age feature has missing values in about 22% of the records. This could be due to:\n\nMissing age information for individuals who did not survive.\nSurvivors possibly being able to provide their age when asked.\n\nThis type of missing data might also be categorized as Missing Not At Random (MNAR) if the likelihood of missing data is related to whether the individual survived or other unobserved factors.\n\n\nEmbarked\nThe Embarked feature has a very small proportion of missing values (0.22%). This is a very minor amount and is likely due to random occurrences.\nSuch a small percentage of missing data is often considered Missing Completely At Random (MCAR), meaning the missingness is unrelated to the observed or unobserved data.\n\n\n\nSummary\n\nCabin and Age features likely fall into the MNAR category due to possible relationships between missingness and other factors like survival status.\nThe Embarked feature’s missing values are likely MCAR, as the missingness appears random and does not correlate with other data aspects.\n\n\n\nB.3. Outliers detection\n\nplt.figure(figsize=(8,6))\nplt.subplot(1, 2, 1)\nfig = train.boxplot(column='Age')\nfig.set_title('')\nfig.set_ylabel('Age')\n\nplt.subplot(1, 2, 2)\nfig = train.boxplot(column='Fare')\nfig.set_title('')\nfig.set_ylabel('Fare')\n\nText(0, 0.5, 'Fare')\n\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(8,6))\n\nplt.subplot(1, 2, 1)\nfig = train.Age.hist(bins=20)\nfig.set_ylabel('Number of passengers')\nfig.set_xlabel('Age')\n\nplt.subplot(1, 2, 2)\nfig = train.Fare.hist(bins=20)\nfig.set_ylabel('Number of passengers')\nfig.set_xlabel('Fare')\n\nText(0.5, 0, 'Fare')\n\n\n\n\n\n\n\n\n\n\n\nB.3. Analyzing the Embarked feature\n\ntrain[train.Embarked.isnull()]\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n61\n62\n1\n1\nIcard, Miss. Amelie\nfemale\n38.0\n0\n0\n113572\n80.0\nB28\nNaN\n\n\n829\n830\n1\n1\nStone, Mrs. George Nelson (Martha Evelyn)\nfemale\n62.0\n0\n0\n113572\n80.0\nB28\nNaN\n\n\n\n\n\n\n\nThe Embarked feature, which records the port of embarkation for passengers, has a very small proportion of missing values (0.22%). This low percentage of missing data suggests a specific pattern in how the data might be missing.\n\nPossible Reasons for Missing Values\n\nConsistency Among Passengers: For passengers who share the same ticket, cabin, and fare, it is unlikely that the missing Embarked data is due to discrepancies in their records. This is because passengers with identical ticket and cabin information would typically have consistent embarkation data.\nData Generation During Dataset Construction: The missing Embarked values could have resulted from data entry or construction processes. For example, if data was manually entered or generated, some records might have been incomplete due to errors or omissions during the data preparation phase.\n\n\n\nNature of Missing Data\nGiven that the missing values in the Embarked feature are minimal and appear to be random rather than systematic, we can categorize this missing data as:\n\nMissing Completely At Random (MCAR): The missingness of the Embarked data is likely unrelated to both the values of the Embarked feature itself and any other features in the dataset. The small percentage of missing data indicates that these omissions do not follow a discernible pattern and are likely due to random errors in data entry or processing.\n\nIn summary, the missing values in the Embarked feature are random and not indicative of any underlying patterns related to the data’s other aspects. This randomness supports the classification of this missing data as MCAR.\n\n\n\nB.4. Analyzing Cabin feature\n\ntrain['cabin_null'] = np.where(train.Cabin.isnull(),1,0)\ntrain.groupby(['Survived'])['cabin_null'].mean()\n\nSurvived\n0    0.876138\n1    0.602339\nName: cabin_null, dtype: float64\n\n\nThe above figures indicates that the missing data is more in the case of passengers not survived(=0).\nThere is a systematic loss of data: people who did not survive tend to have more information missing. Presumably, the method chosen to gather the information, contributes to the generation of these missing data.\n\n\nB.5. Analyzing the Age feature\n\ntrain['age_null'] = np.where(train.Age.isnull(),1,0)\ntrain.groupby(['Survived'])['age_null'].mean()\n\nSurvived\n0    0.227687\n1    0.152047\nName: age_null, dtype: float64\n\n\nThere is a systematic loss of data: people who did not survive tend to have more information missing. Presumably, the method chosen to gather the information, contributes to the generation of these missing data.\n\n\nB.6. Analyzing the Fare feature\nThe distribution of Fare is skewed, so in principle, we shouldn’t estimate outliers using the mean plus minus 3 standard deviations methods, which assumes a normal distribution of the data.\n\ntotal_passengers = float(train.shape[0])\n\nprint('Total number of passengers: {}'.format(train.shape[0]))\nprint('Passengers that paid more than 65: {:.2f}%'.format(\n    (train[train.Fare &gt; 65].shape[0] / total_passengers) * 100))\nprint('passengers that paid more than 100: {} %'.format((\n    train[train.Fare &gt; 100].shape[0]/ total_passengers)*100))\n\nTotal number of passengers: 891\nPassengers that paid more than 65: 13.02%\npassengers that paid more than 100: 5.948372615039282 %\n\n\nThere is unusual high values of Fares observed, the reason is found as follows:\n\n#at the most extreme outliers\ntrain[train.Fare&gt;300]\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\ncabin_null\nage_null\n\n\n\n\n258\n259\n1\n1\nWard, Miss. Anna\nfemale\n35.0\n0\n0\nPC 17755\n512.3292\nNaN\nC\n1\n0\n\n\n679\n680\n1\n1\nCardeza, Mr. Thomas Drake Martinez\nmale\n36.0\n0\n1\nPC 17755\n512.3292\nB51 B53 B55\nC\n0\n0\n\n\n737\n738\n1\n1\nLesurer, Mr. Gustave J\nmale\n35.0\n0\n0\nPC 17755\n512.3292\nB101\nC\n0\n0\n\n\n\n\n\n\n\nThese three people have the same ticket number, indicating that they were travelling together. The Fare price in this case, 512 is the price of 3 tickets, and not one. This is why, it is unusually high.\n\nB.7. Categorical Values :\n\nprint('Number of categories in the variable Name: {}'.format(\n    len(train.Name.unique())))\n\nprint('Number of categories in the variable Gender: {}'.format(\n    len(train.Sex.unique())))\n\nprint('Number of categories in the variable Ticket: {}'.format(\n    len(train.Ticket.unique())))\n\nprint('Number of categories in the variable Cabin: {}'.format(\n    len(train.Cabin.unique())))\n\nprint('Number of categories in the variable Embarked: {}'.format(\n    len(train.Embarked.unique())))\n\nprint('Total number of passengers in the Titanic: {}'.format(len(train)))\n\nNumber of categories in the variable Name: 891\nNumber of categories in the variable Gender: 2\nNumber of categories in the variable Ticket: 681\nNumber of categories in the variable Cabin: 148\nNumber of categories in the variable Embarked: 4\nTotal number of passengers in the Titanic: 891\n\n\n\ndrop_column = ['cabin_null','age_null']\ntrain.drop(drop_column , axis =1  ,inplace = True )"
  },
  {
    "objectID": "posts/kaggle/titanic-survival-eda-feature-engineering-preds.html#c.-feature-scaling-and-engineering",
    "href": "posts/kaggle/titanic-survival-eda-feature-engineering-preds.html#c.-feature-scaling-and-engineering",
    "title": "Titanic Survival Prediction",
    "section": "C. Feature Scaling and Engineering",
    "text": "C. Feature Scaling and Engineering\nFeature scaling is a technique used to standardize the range of independent variables or features of data. In machine learning and data analysis, scaling is important because it helps improve the performance and training stability of models.\n\nC.1. Handling the Missing Values:\nThe dataset contains missing values in several features. To address these, we apply different strategies based on the nature of each feature:\n\ntrain.isnull().sum()\n\nPassengerId      0\nSurvived         0\nPclass           0\nName             0\nSex              0\nAge            177\nSibSp            0\nParch            0\nTicket           0\nFare             0\nCabin          687\nEmbarked         2\ndtype: int64\n\n\n\ntest.isnull().sum()\n\nPassengerId      0\nPclass           0\nName             0\nSex              0\nAge             86\nSibSp            0\nParch            0\nTicket           0\nFare             1\nCabin          327\nEmbarked         0\ndtype: int64\n\n\n\ndata_cleaner = [test , train]\nfor dataset in data_cleaner:    \n    #completing missing age with median\n    dataset['Age'].fillna(dataset['Age'].median(), inplace = True)\n\n    #completing embarked with mode\n    dataset['Embarked'].fillna(dataset['Embarked'].mode()[0], inplace = True)\n\n    #completing missing fare with median\n    dataset['Fare'].fillna(dataset['Fare'].median(), inplace = True)\n    \n#delete the train feature\ntrain.drop(['Ticket'], axis=1, inplace = True)\ntest.drop(['Ticket'] , axis=1 , inplace = True)\n\n\n\nC.2. Encoding\nEncoding is a crucial step in data preprocessing, especially for machine learning and statistical modeling. It involves converting categorical variables (features that represent categories or groups) into numerical values that can be processed by machine learning algorithms.\n\nC.2.1. Cabin Feature\n\ndrop_column = ['Cabin']\ntrain.drop(drop_column , axis =1  ,inplace = True )\ntest.drop(drop_column , axis =1  ,inplace = True )\n\nThe Cabin feature has been dropped from the dataset due to its high proportion of missing values (77.10%), which makes it less informative.\n\n\nC.2.2. Fare Feature\n\nfull_data = [train,test]\nfor dataset in full_data:\n    dataset.loc[ dataset['Fare'] &lt;= 7.91, 'Fare_Band'] = 0\n    dataset.loc[(dataset['Fare'] &gt; 7.91) & (dataset['Fare'] &lt;= 14.454), 'Fare_Band'] = 1\n    dataset.loc[(dataset['Fare'] &gt; 14.454) & (dataset['Fare'] &lt;= 31), 'Fare_Band'] = 2\n    dataset.loc[ dataset['Fare'] &gt; 31, 'Fare_Band'] = 3\n    dataset['Fare_Band'] = dataset['Fare_Band'].astype(int)\n    dataset.drop(['Fare' ], axis = 1 , inplace =True)\n\nThe Fare feature has been transformed into discrete fare bands. This transformation categorizes fare amounts into bins, which can simplify the modeling process and potentially reveal patterns.\n\n\nC.2.3. Age Feature\n\nfull_data = [test , train]\nfor dataset in full_data:\n    \n    dataset.loc[ dataset['Age'] &lt;= 10, 'Age'] = 0\n    dataset.loc[(dataset['Age'] &gt; 10) & (dataset['Age'] &lt;= 15), 'Age'] = 1\n    dataset.loc[(dataset['Age'] &gt; 15) & (dataset['Age'] &lt;= 20), 'Age'] = 2\n    dataset.loc[(dataset['Age'] &gt; 20) & (dataset['Age'] &lt;= 25), 'Age'] = 3\n    dataset.loc[(dataset['Age'] &gt; 25) & (dataset['Age'] &lt;= 30), 'Age'] = 4\n    dataset.loc[(dataset['Age'] &gt; 30) & (dataset['Age'] &lt;= 45), 'Age'] = 5\n    dataset.loc[(dataset['Age'] &gt; 45) & (dataset['Age'] &lt;= 60), 'Age'] = 6\n    dataset.loc[ dataset['Age'] &gt; 60, 'Age'] = 7\n    dataset['Age'] = dataset['Age'].astype(int)\n\nThe Age feature has been converted into age bins, categorizing age into discrete intervals. This transformation simplifies the feature and can help capture age-related patterns more effectively.\n\n\nC.2.4. Sex and Embarked Feature\n\nfull_data = [test , train]\nfor dataset in full_data:\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n    dataset['Sex'] = dataset['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n\nThe categorical features Embarked and Sex have been encoded into numeric values. This encoding converts categorical variables into a format suitable for machine learning models.\n\n\nC.2.5. Droping the Name feature\n\ntrain.drop(['Name'],axis = 1, inplace = True)\ntest.drop(['Name'],axis = 1, inplace = True )\n\nThe Name feature, which does not provide useful information for modeling, has been removed from both the training and testing datasets.\n\n\nC.2.6. Family Size\n\ntrain['family_size'] = train['SibSp'] + train['Parch'] + 1 \ntest['family_size'] = test['SibSp'] + test['Parch'] + 1 \ntest['IsAlone'] = 1 \ntrain['IsAlone'] = 1 \ntrain['IsAlone'].loc[train['family_size'] &gt; 1] = 0\ntest['IsAlone'].loc[test['family_size'] &gt; 1] = 0 \ntest.drop(['SibSp' , 'Parch'], axis = 1 , inplace =True)\ntrain.drop(['SibSp','Parch' ], axis = 1 , inplace =True)\n\nA new feature, family_size, is created by combining SibSp (siblings/spouses aboard) and Parch (parents/children aboard). This feature provides insight into the size of the family traveling with the passenger.\n\ntest.isnull().sum()\n\nPassengerId    0\nPclass         0\nSex            0\nAge            0\nEmbarked       0\nFare_Band      0\nfamily_size    0\nIsAlone        0\ndtype: int64"
  },
  {
    "objectID": "posts/kaggle/titanic-survival-eda-feature-engineering-preds.html#d.-visualizations",
    "href": "posts/kaggle/titanic-survival-eda-feature-engineering-preds.html#d.-visualizations",
    "title": "Titanic Survival Prediction",
    "section": "D. Visualizations",
    "text": "D. Visualizations\n\nLet’s get some insights !\n\ng = sns.FacetGrid(train, col=\"Survived\", row=\"Sex\", hue=\"Embarked\", height=3)\ng.map(plt.hist, \"Pclass\", edgecolor=\"w\").add_legend()\n\n\n\n\n\n\n\n\n\nObservations\n\nFrom above graph we observe that more number of females survived as compared to males. The female survivors were more from the first class and male from third class were the most to die.\nThe 3rd class people were the most affected, that is they less survived where as 1st class people survived is maximum than others.\nThe second class has almost equal survived and couldn’t survive number of people. And also we notice many of the passengers Embarked from “S”.\n\n\n\nplt.figure(figsize = [8,5])\nsns.violinplot(x=\"Fare_Band\", y=\"Age\", data=train, hue='Survived',palette='coolwarm')\n\n\n\n\n\n\n\n\nMostly farebands are greater at the Age Group “4”. Survival also has greater area corresponding to age group “4”.\n\ntrain[['family_size', 'Survived']].groupby(['family_size'], as_index=False).mean()\n\n\n\n\n\n\n\n\nfamily_size\nSurvived\n\n\n\n\n0\n1\n0.303538\n\n\n1\n2\n0.552795\n\n\n2\n3\n0.578431\n\n\n3\n4\n0.724138\n\n\n4\n5\n0.200000\n\n\n5\n6\n0.136364\n\n\n6\n7\n0.333333\n\n\n7\n8\n0.000000\n\n\n8\n11\n0.000000\n\n\n\n\n\n\n\n\naxes = sns.catplot(x='family_size', y='Survived', hue='Sex', data=train, aspect=3, kind='point')\n\n\n\n\n\n\n\n\nWe find with increase in family size the survival rate decreases.\n\nplt.figure(figsize=(10,10))\nsns.heatmap(train.drop('PassengerId',axis=1).corr(), square=True, annot=True)\n\n\n\n\n\n\n\n\n\nUndestanding the Correlation matrix:\n\nThe FareBand and Pclass are highly correlated(-0.63) although negative, next to them is FareBand and IsAlone correlation(-0.57).\nThe Sex and Survived also have good correlation of (-0.54).\nBut as observed IsAlone and Family_size has the largest negative correlation (-0.69) is liable as the Family size and being alone are two opposite categories."
  },
  {
    "objectID": "posts/kaggle/titanic-survival-eda-feature-engineering-preds.html#e.-model-training-and-predicting",
    "href": "posts/kaggle/titanic-survival-eda-feature-engineering-preds.html#e.-model-training-and-predicting",
    "title": "Titanic Survival Prediction",
    "section": "E. Model Training and Predicting",
    "text": "E. Model Training and Predicting\n\nSpitting the data in ro train and test\n\nX = train.drop('Survived' , axis = 1 )\ny = train['Survived']\n\nfrom sklearn.model_selection import train_test_split\nX_train ,X_test , y_train , y_test = train_test_split(X , y , test_size = 0.3 , random_state =102)\n\nAlso we need to remove Id of passengers for prediction,\n\nX_train=X_train.drop(['PassengerId'],axis=1)\nX_test = X_test.drop(['PassengerId'],axis=1)\n\nImporting models from scikit learn module. The objective is to classify the passenger survivior into two classes: 0 or 1, hence this is a binary classification for which we will be using classifiers. Following part of this notebook compares and finds the best model suitable for the data based upon accuracy metrics.\n\n#Importing all models\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score\n\n\nE.1. Logistic Regression\n\nlogmodel = LogisticRegression()\nlogmodel.fit(X_train , y_train)\npred_l = logmodel.predict(X_test)\nacc_l = accuracy_score(y_test , pred_l)*100\nacc_l\n\n79.1044776119403\n\n\n\n\nE.2. Random Forest\n\nrandom_forest = RandomForestClassifier(n_estimators= 100)\nrandom_forest.fit(X_train, y_train)\npred_rf = random_forest.predict(X_test)\nacc_rf = accuracy_score(y_test , pred_rf)*100\nacc_rf\n\n81.71641791044776\n\n\n\n\nE.3. K-Nearest Neighbours\n\nknn = KNeighborsClassifier(n_neighbors = 3)\n\nknn.fit(X_train, y_train)\n\npred_knn = knn.predict(X_test)\n\nacc_knn = accuracy_score(y_test , pred_knn)*100\nacc_knn\n\n79.8507462686567\n\n\n\n\nE.4. Gaussian Naive Bayes Classifier\n\ngaussian = GaussianNB()\n\ngaussian.fit(X_train, y_train)\n\npred_gb = gaussian.predict(X_test)\n\nacc_gb = accuracy_score(y_test , pred_gb)*100\nacc_gb\n\n77.98507462686567\n\n\n\n\nE.5. C-Support Vector Classifier\n\nsvc = SVC()\n\nsvc.fit(X_train, y_train)\n\npred_svc = svc.predict(X_test)\n\nacc_svc = accuracy_score(y_test , pred_svc)*100\nacc_svc\n\n84.70149253731343\n\n\n\n\nE.6. Decision Tree\n\ndecision_tree = DecisionTreeClassifier()\n\ndecision_tree.fit(X_train, y_train)\n\npred_dt = decision_tree.predict(X_test)\n\nacc_dt = accuracy_score(y_test , pred_dt)*100\nacc_dt\n\n81.34328358208955\n\n\n\n\nE.7. Linear classifiers with SGD training.\n\nsgd = SGDClassifier()\n\nsgd.fit(X_train, y_train)\n\npred_sgd = sgd.predict(X_test)\n\nacc_sgd = accuracy_score(y_test , pred_sgd)*100\nacc_sgd\n\n68.65671641791045\n\n\n\n## Arranging the Accuracy results\nmodels = pd.DataFrame({\n    'Model': ['Logistic Regression', 'Random Forrest','K- Nearest Neighbour' ,\n             'Naive Bayes' , 'C-Support Vector Classifier' , 'Decision Tree' , 'Stochastic Gradient Descent'],\n    'Score': [acc_l , acc_rf , acc_knn , acc_gb , acc_svc , \n              acc_dt , acc_sgd]})\nmodels.sort_values(by='Score', ascending=False)\n\n\n\n\n\n\n\n\nModel\nScore\n\n\n\n\n4\nC-Support Vector Classifier\n84.701493\n\n\n1\nRandom Forrest\n81.716418\n\n\n5\nDecision Tree\n81.343284\n\n\n2\nK- Nearest Neighbour\n79.850746\n\n\n0\nLogistic Regression\n79.104478\n\n\n3\nNaive Bayes\n77.985075\n\n\n6\nStochastic Gradient Descent\n68.656716"
  },
  {
    "objectID": "posts/kaggle/titanic-survival-eda-feature-engineering-preds.html#ensemble-learning",
    "href": "posts/kaggle/titanic-survival-eda-feature-engineering-preds.html#ensemble-learning",
    "title": "Titanic Survival Prediction",
    "section": "Ensemble Learning",
    "text": "Ensemble Learning\n\ndf_test =  test.drop(['PassengerId'],axis=1)\n\np_l = logmodel.predict(df_test)\np_svc = svc.predict(df_test)\np_rf = random_forest.predict(df_test)\np_dt = decision_tree.predict(df_test)\n\n\npredict_combine = np.zeros((df_test.shape[0]))\nfor i in range(0, test.shape[0]):\n    temp = p_rf[i]+p_svc[i]+p_l[i]+p_dt[i]\n    if temp&gt;=2:\n        predict_combine[i] = 1\npredict_combine = predict_combine.astype('int')"
  },
  {
    "objectID": "posts/kaggle/titanic-survival-eda-feature-engineering-preds.html#submission",
    "href": "posts/kaggle/titanic-survival-eda-feature-engineering-preds.html#submission",
    "title": "Titanic Survival Prediction",
    "section": "Submission",
    "text": "Submission\n\nsubmission = pd.DataFrame({\n       \"PassengerId\": test[\"PassengerId\"],\n        \"Survived\": predict_combine\n    })\n\nsubmission.to_csv(\"submission.csv\", encoding='utf-8', index=False)\n\n\nsubmission.head()\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\n\n\n\n\n0\n892\n0\n\n\n1\n893\n0\n\n\n2\n894\n0\n\n\n3\n895\n0\n\n\n4\n896\n1"
  },
  {
    "objectID": "posts/r-package-mgt/r-package-mgt.html",
    "href": "posts/r-package-mgt/r-package-mgt.html",
    "title": "Simplifying R Package Management - renv vs packrat",
    "section": "",
    "text": "In this blog post, we’ll explore two popular tools for R package management – renv and packrat. We’ll delve into their features, advantages, and provide practical examples to help you make an informed decision."
  },
  {
    "objectID": "posts/r-package-mgt/r-package-mgt.html#the-conundrum-of-package-management",
    "href": "posts/r-package-mgt/r-package-mgt.html#the-conundrum-of-package-management",
    "title": "Simplifying R Package Management - renv vs packrat",
    "section": "The Conundrum of Package Management",
    "text": "The Conundrum of Package Management\nInstalling R packages is not always a walk in the park. Packages sourced from repositories such as CRAN, BiocManager, or GitHub can introduce complexity, involving multiple OS dependencies. This complexity magnifies during installation, making it challenging to manage, reproduce, and keep track of installed packages for a specific project. Enter package management tools, designed to streamline this process."
  },
  {
    "objectID": "posts/r-package-mgt/r-package-mgt.html#meet-renv",
    "href": "posts/r-package-mgt/r-package-mgt.html#meet-renv",
    "title": "Simplifying R Package Management - renv vs packrat",
    "section": "Meet renv",
    "text": "Meet renv\n\nWhat is renv?\nrenv is a package management tool for R that focuses on project-specific isolation. It allows you to create a self-contained environment for your R project, capturing and managing dependencies effectively.\n\n\nKey Features of renv\n\nIsolation: renv creates a dedicated project library, ensuring that each project has its own set of packages, eliminating conflicts between projects.\nSnapshotting: With renv, you can create a snapshot of your project’s dependencies, making it easy to replicate the environment on another machine.\nVersion Control Integration: renv integrates seamlessly with version control systems, ensuring reproducibility across different development environments.\n\n\n\nGetting Started with renv\n# Install renv\ninstall.packages(\"renv\")\n\n# Initialize renv in your project\nlibrary(renv)\nrenv::init()\n\n# Install and snapshot dependencies\nrenv::install(\"package_name\")\nrenv::snapshot()"
  },
  {
    "objectID": "posts/r-package-mgt/r-package-mgt.html#how-to-reproduce-renv-envrionment",
    "href": "posts/r-package-mgt/r-package-mgt.html#how-to-reproduce-renv-envrionment",
    "title": "Simplifying R Package Management - renv vs packrat",
    "section": "How to reproduce renv envrionment ?",
    "text": "How to reproduce renv envrionment ?\nThe above snapshot step produces a renv.lock file that is basically a record of the exact package versions and dependencies used in your project. This renv.lock file is your golden ticket to reproducing the renv environment on another machine or at a later time. Here’s how you can effortlessly replicate your renv environment:\n\nReproducing the renv Environment\n\nSharing the renv.lock File: Share the renv.lock file with your collaborators or store it in your version control system (e.g., Git). This file acts as a precise blueprint of your project’s dependencies.\nInitialization on a New Machine: On a new machine or for a different developer, start by cloning your project repository (if using version control). Navigate to the project directory and open an R session.\n\n# Install renv (if not already installed)\ninstall.packages(\"renv\")\n\n# Initialize renv in the project\nlibrary(renv)\nrenv::init()\n\n# Restore the environment using the lock file\nrenv::restore()\n\nInstalling Packages: Once the environment is restored, install the required packages using the snapshot.\n\n# Install packages from the lock file\nrenv::install()"
  },
  {
    "objectID": "posts/r-package-mgt/r-package-mgt.html#example-renv.lock-file",
    "href": "posts/r-package-mgt/r-package-mgt.html#example-renv.lock-file",
    "title": "Simplifying R Package Management - renv vs packrat",
    "section": "Example renv.lock file",
    "text": "Example renv.lock file\nIn the following example:\n\nThe “R” section specifies the R version used in the project.\nThe “Packages” section lists the packages used, each with its name, version, and source (e.g., CRAN).\nThe “Dependencies” section outlines the dependencies for each package, including both Imports and LinkingTo.\n\n{\n  \"R\": {\n    \"Version\": \"4.2.0\"\n  },\n  \"Packages\": {\n    \"data.table\": {\n      \"Package\": \"data.table\",\n      \"Version\": \"1.14.0\",\n      \"Source\": \"CRAN\"\n    },\n    \"ggplot2\": {\n      \"Package\": \"ggplot2\",\n      \"Version\": \"3.3.5\",\n      \"Source\": \"CRAN\"\n    }\n  },\n  \"Dependencies\": {\n    \"data.table\": {\n      \"Imports\": [],\n      \"LinkingTo\": []\n    },\n    \"ggplot2\": {\n      \"Imports\": [\n        \"methods\",\n        \"grDevices\",\n        \"graphics\",\n        \"stats\"\n      ],\n      \"LinkingTo\": []\n    }\n  }\n}"
  },
  {
    "objectID": "posts/r-package-mgt/r-package-mgt.html#exploring-packrat",
    "href": "posts/r-package-mgt/r-package-mgt.html#exploring-packrat",
    "title": "Simplifying R Package Management - renv vs packrat",
    "section": "Exploring packrat",
    "text": "Exploring packrat\n\nWhat is packrat?\npackrat is another robust package management tool for R. It addresses the challenges of package management by providing project-specific libraries and snapshot capabilities.\n\n\nKey Features of packrat\n\nBundled Library: packrat creates a local library for each project, ensuring that packages are self-contained within the project directory.\nSnapshotting: Similar to renv, packrat enables you to create a snapshot of your project’s dependencies, promoting reproducibility.\nIntegration with IDEs: packrat integrates seamlessly with popular R IDEs, making it convenient for developers who rely on specific development environments.\n\n\n\nGetting Started with packrat\n# Install packrat\ninstall.packages(\"packrat\")\n\n# Initialize packrat in your project\nlibrary(packrat)\npackrat::init()\n\n# Install and snapshot dependencies\npackrat::install(\"package_name\")\npackrat::snapshot()"
  },
  {
    "objectID": "posts/r-package-mgt/r-package-mgt.html#choosing-the-right-tool-for-you",
    "href": "posts/r-package-mgt/r-package-mgt.html#choosing-the-right-tool-for-you",
    "title": "Simplifying R Package Management - renv vs packrat",
    "section": "Choosing the Right Tool for You",
    "text": "Choosing the Right Tool for You\nBoth renv and packrat offer powerful solutions to the challenges of R package management. Your choice may depend on personal preference, project requirements, or team workflows. Experiment with both tools and consider factors like ease of use, integration capabilities, and community support.\nEffective package management is the cornerstone of reproducibility and collaboration in R development. Choose wisely, and may your R projects flourish without the headaches of package chaos!"
  },
  {
    "objectID": "posts/r-package-mgt/r-package-mgt.html#references",
    "href": "posts/r-package-mgt/r-package-mgt.html#references",
    "title": "Simplifying R Package Management - renv vs packrat",
    "section": "References",
    "text": "References\n\nrenv\npackrat\nrenv vs packrat"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Pratik Kumar",
    "section": "",
    "text": "Hello! 👋 I am a software engineer who loves working on websites, AI, and data-related projects. When I’m not coding, I enjoy going on treks, exploring new cuisines, and playing sports like football, badminton, cricket, and table tennis. I love photography and watching exciting movies like “Edge of Tomorrow”. And I am always up for a coffee !\nCurrently, I am working at Elucidata, solving some of the complex problems in drug discovery.\nView my resume here."
  },
  {
    "objectID": "about.html#work-experience",
    "href": "about.html#work-experience",
    "title": "Pratik Kumar",
    "section": "👨🏻‍💻 Work Experience",
    "text": "👨🏻‍💻 Work Experience\n\nElucidata | Aug 2021 - Present  - Software Engineer (current)  - Data Analyst\nElucidata | May 2021 - Aug 2021  - Data Science Intern\nBayer Cropscience | May 2019 - June 2019  - Data Science Intern"
  },
  {
    "objectID": "posts/hierarchical/Plotly-Sunburst.html",
    "href": "posts/hierarchical/Plotly-Sunburst.html",
    "title": "Hierarchical Data Visualization Demystified",
    "section": "",
    "text": "Data visualization plays a vital role in various domains such as data analytics, data science, data dashboarding, and exploratory/statistical analysis. Within the Python and R ecosystems, there are several popular visualization libraries commonly used such as :\n\nMatplotlib\nSeaborn\nPlotly\nAltair\nBokeh\n\nAmong these, the widely used library is the Plotly Graphing Library, which offers libraries in multiple languages, high-quality scientific/non-scientific graphs, and easily shareable interactive plots.\nIn this post, I will be discussing an intriguing plot called the Sunburst Chart. Sunburst charts provide an interactive visualization of layered information, allowing for an enhanced understanding of complex data structures."
  },
  {
    "objectID": "posts/hierarchical/Plotly-Sunburst.html#introduction",
    "href": "posts/hierarchical/Plotly-Sunburst.html#introduction",
    "title": "Hierarchical Data Visualization Demystified",
    "section": "",
    "text": "Data visualization plays a vital role in various domains such as data analytics, data science, data dashboarding, and exploratory/statistical analysis. Within the Python and R ecosystems, there are several popular visualization libraries commonly used such as :\n\nMatplotlib\nSeaborn\nPlotly\nAltair\nBokeh\n\nAmong these, the widely used library is the Plotly Graphing Library, which offers libraries in multiple languages, high-quality scientific/non-scientific graphs, and easily shareable interactive plots.\nIn this post, I will be discussing an intriguing plot called the Sunburst Chart. Sunburst charts provide an interactive visualization of layered information, allowing for an enhanced understanding of complex data structures."
  },
  {
    "objectID": "posts/hierarchical/Plotly-Sunburst.html#sunburst-chart",
    "href": "posts/hierarchical/Plotly-Sunburst.html#sunburst-chart",
    "title": "Hierarchical Data Visualization Demystified",
    "section": "Sunburst Chart",
    "text": "Sunburst Chart\nA sunburst chart is a powerful visualization tool used to represent hierarchical datasets. In a hierarchical dataset, there exists a parent-child relationship among the features or variables, resembling a tree-like structure. To generate a sunburst plot using Plotly, you can leverage the capabilities of either plotly.express or plotly.graph_objects libraries.\nNow, let’s delve into how this data would appear by visualizing it using a sunburst chart."
  },
  {
    "objectID": "posts/hierarchical/Plotly-Sunburst.html#hierarchical-data",
    "href": "posts/hierarchical/Plotly-Sunburst.html#hierarchical-data",
    "title": "Hierarchical Data Visualization Demystified",
    "section": "Hierarchical Data",
    "text": "Hierarchical Data\nHierarchical datasets are a type of data organization where the data is structured in a hierarchical manner, forming a tree-like structure. In this structure, data elements are grouped into parent-child relationships, where each parent can have one or more children, and each child can be a parent of other elements, forming multiple levels of nesting.\nConsider an example dataframe (dummy data for demonstration purposes) with a tree-like structure, where the columns or features exhibit parent-child relationships with other columns.\n\nGeneral Dataset: This dataframe contains classes and values organized in columns, as depicted in the sample data provided.\nSunburst Dataset: This hierarchical dataframe defines the logical parent-child relationships between columns and their corresponding values.\n\nThe following dataset is a dummy data for demonstration.\n\n#Importing pandas to handle dataframe\nimport pandas as pd\n# Suppress pandas warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\ndata = pd.read_csv(\"../data/dummy_data.csv\")\ndata.head(5)\n\n\n\n\n\n\n\n\nCountry\nState\nCity\nPopulation\n\n\n\n\n0\nIndia\nINMP\nA1\n512\n\n\n1\nIndia\nINCG\nB2\n12201\n\n\n2\nIndia\nINCG\nM1\n9021\n\n\n3\nUSA\nUSNY\nC2\n812\n\n\n4\nUSA\nUSNY\nN1\n821\n\n\n\n\n\n\n\nThe dataset is not in hierachical form. The sunburst chart needs a parent, child and value variable for generating the plot. Hence, we need to convert the table into a ‘chart-acceptable’ format. The following function performs the job. The function is modified version of original function defined at Plotly’s documentation, to know more about this please visit here.\n\ndef build_hierarchical_dataframe(df, levels, value_column, metric):\n    \"\"\"\n    Build a hierarchy of levels for Sunburst.\n    \n    Args:\n        df (pd.DataFrame): Input pandas DataFrame.\n        levels (list): List of column names in the order, child to root.\n        value_column (str): Name of the column to display in the chart.\n        metric (str): Specifies the metric, either \"sum\" or \"count\".\n        \n    Returns:\n        pd.DataFrame: A pandas DataFrame for Sunburst with columns ['id', 'parent', 'value'].\n    \"\"\"\n    df_all_trees = pd.DataFrame(columns=['id', 'parent', 'value'])\n    \n    for i, level in enumerate(levels):\n        df_tree = pd.DataFrame(columns=['id', 'parent', 'value'])\n        \n        # Groupby based on the chosen metric\n        if metric == \"count\":\n            dfg = df.groupby(levels[i:]).count()\n        else:\n            dfg = df.groupby(levels[i:]).sum()\n        \n        dfg = dfg.reset_index()\n        df_tree['id'] = dfg[level].copy()\n\n        # Set parent of the levels\n        if i &lt; len(levels) - 1:\n            df_tree['parent'] = dfg[levels[i+1]].copy()\n        else:\n            df_tree['parent'] = 'Total'\n        \n        df_tree['value'] = dfg[value_column]\n        df_all_trees = pd.concat([df_all_trees, df_tree], ignore_index=True)\n    \n    # Value calculation for the parent\n    if metric == \"count\":\n        total = pd.Series(dict(id='Total', parent='', value=df[value_column].count()))\n    else:\n        total = pd.Series(dict(id='Total', parent='', value=df[value_column].sum()))\n    \n    # Add frames one below the other to form the final dataframe\n    df_all_trees = pd.concat([df_all_trees, pd.DataFrame([total])], ignore_index=True)\n    return df_all_trees\n\n\nlevels = ['City', 'State', 'Country'] \nvalue_column = 'Population'\n\n\n(1) Hierarchical Sum dataframe\nThis dataframe represents total population accross Country, State and City under study.\n\ndf_sum=build_hierarchical_dataframe(data, levels, value_column, metric=\"sum\")\ndf_sum.head()\n\n\n\n\n\n\n\n\nid\nparent\nvalue\n\n\n\n\n0\nA1\nINMP\n512\n\n\n1\nB2\nINCG\n12201\n\n\n2\nC2\nUSNY\n812\n\n\n3\nD1\nINSD\n9104\n\n\n4\nE2\nINGD\n132\n\n\n\n\n\n\n\n\n\n(2) Hierarchical Count dataframe\nThis dataframe represents number of sub-classes (like City) accross Country and State under study.\n\ndf_count=build_hierarchical_dataframe(data, levels, value_column, metric=\"count\")\ndf_count.head()\n\n\n\n\n\n\n\n\nid\nparent\nvalue\n\n\n\n\n0\nA1\nINMP\n1\n\n\n1\nB2\nINCG\n1\n\n\n2\nC2\nUSNY\n1\n\n\n3\nD1\nINSD\n1\n\n\n4\nE2\nINGD\n1"
  },
  {
    "objectID": "posts/hierarchical/Plotly-Sunburst.html#visualisation",
    "href": "posts/hierarchical/Plotly-Sunburst.html#visualisation",
    "title": "Hierarchical Data Visualization Demystified",
    "section": "Visualisation",
    "text": "Visualisation\nNow we would see the two most common ways of plotting sunburst charts in python. The user can choose any of the following modules,\n\nPlotly Express\nPlotly Graph Objects\n\nBoth of these modules generate same “figure object”. Just the difference comes in code syntax and in flexibility of modifying graph as required. Plotly express is more of generating plot by calling function from already defined set of parameters. One may be more comfortable in tweaking the details while working with graph objects. However, the beauty of plotly is that you are able do the same things in the figure generated from plotly express as those are possible in that with graph objects. \nWe will be using both of them, and generate the plots for the datasets generated in the above section.\n\nfrom io import StringIO\nfrom IPython.display import display_html, HTML\n\n\n(1) Plotly Express\n\nimport plotly.express as px \n\nfigure = px.sunburst(data, path=['Country', 'State', 'City'], values='Population')\nfigure.update_layout(margin=dict(t=10, b=10, r=10, l=10))\nfigure.show() \n# HTML(figure.to_html(include_plotlyjs='cdn'))\n\n                                                \n\n\n\n\n(2) Graph Objects\n\nimport plotly.graph_objects as go\n\nfigure = go.Figure()\nfigure.add_trace(go.Sunburst(\n        labels=df_sum['id'],\n        parents=df_sum['parent'],\n        values=df_sum['value'],\n        branchvalues='total',\n        marker=dict(colorscale='Rdbu'),\n        hovertemplate='&lt;b&gt; Country : %{label} &lt;/b&gt; &lt;br&gt; Count : %{value} &lt;extra&gt;Population&lt;/extra&gt;',\n        maxdepth=2)\n    )\nfigure.update_layout(margin=dict(t=10, b=10, r=10, l=10))\nfigure.show() \n# HTML(figure.to_html(include_plotlyjs='cdn'))"
  },
  {
    "objectID": "posts/hierarchical/Plotly-Sunburst.html#communicating-plots-with-json",
    "href": "posts/hierarchical/Plotly-Sunburst.html#communicating-plots-with-json",
    "title": "Hierarchical Data Visualization Demystified",
    "section": "Communicating Plots with JSON",
    "text": "Communicating Plots with JSON\nWe can take these plots and convert them to JSONs. This comes handy when we need the plots to communicate from server part of a web application to client. Plotly has in-built function to save figure as json : write_json(). Following cells show how to write and regenerate the plots.\n\nfigure.write_json(\"../data/Sunburst_Chart.json\")\n\n\nimport json\n\nopened_file = open(\"../data/Sunburst_Chart.json\")\nopened_fig = json.load(opened_file)\n\nfig_ = go.Figure(\n    data = opened_fig['data'],\n    layout = opened_fig['layout']\n    )\nfig_.show()\n# HTML(fig_.to_html())"
  },
  {
    "objectID": "posts/hierarchical/Plotly-Sunburst.html#custom-plots",
    "href": "posts/hierarchical/Plotly-Sunburst.html#custom-plots",
    "title": "Hierarchical Data Visualization Demystified",
    "section": "Custom Plots",
    "text": "Custom Plots\nIn this final section we would see the go.Figure subplots, where fully customize the plots.\n\nfrom plotly.subplots import make_subplots\n\nfig = make_subplots(1, 2, specs=[[{\"type\": \"domain\"}, {\"type\": \"domain\"}]],)\nfig.add_trace(go.Sunburst(\n    labels=df_sum['id'],\n    parents=df_sum['parent'],\n    values=df_sum['value'],\n    branchvalues='total',\n    marker=dict(colorscale='sunset'),\n    hovertemplate='&lt;b&gt; Country : %{label} &lt;/b&gt; &lt;br&gt; Count : %{value} &lt;extra&gt;Population&lt;/extra&gt;',\n    maxdepth=2), 1, 1)\n\nfig.add_trace(go.Sunburst(\n    labels=df_count['id'],\n    parents=df_count['parent'],\n    values=df_count['value'],\n    branchvalues='total',\n    marker=dict(colorscale='viridis'),\n    hovertemplate='&lt;b&gt; Country : %{label} &lt;/b&gt; &lt;br&gt; Count : %{value} &lt;extra&gt;Cities&lt;/extra&gt;',\n    maxdepth=2), 1, 2)\n\nfig.update_layout(margin=dict(t=10, b=10, r=10, l=10))\nfig.show()\n# HTML(fig.to_html())"
  },
  {
    "objectID": "posts/hierarchical/Plotly-Sunburst.html#only-sunburst-what-are-some-alternatives-to-sunburst",
    "href": "posts/hierarchical/Plotly-Sunburst.html#only-sunburst-what-are-some-alternatives-to-sunburst",
    "title": "Hierarchical Data Visualization Demystified",
    "section": "Only Sunburst ? What are some alternatives to Sunburst ?",
    "text": "Only Sunburst ? What are some alternatives to Sunburst ?\nSunburst is one of the ways of visualizing the Hierarchical Data, we can also visualize such datasets using Treemap charts. For example -\n\nfig = px.treemap(data, \n                 path=[px.Constant(\"World\"), 'Country', 'State', 'City'], \n                 values='Population')\nfig.update_layout(margin = dict(t=50, l=25, r=25, b=25))\nfig.show()"
  },
  {
    "objectID": "posts/kaggle/loan-status-eda-classification.html",
    "href": "posts/kaggle/loan-status-eda-classification.html",
    "title": "Loan Status Prediction",
    "section": "",
    "text": "The goal of this project is to develop an automated system for predicting loan eligibility based on customer details provided through an online application form. The company aims to streamline and optimize their loan approval process by leveraging data to identify customer segments that are most likely to be eligible for a loan. This will enable targeted marketing and more efficient processing of applications."
  },
  {
    "objectID": "posts/kaggle/loan-status-eda-classification.html#importing-libraries",
    "href": "posts/kaggle/loan-status-eda-classification.html#importing-libraries",
    "title": "Loan Status Prediction",
    "section": "Importing Libraries",
    "text": "Importing Libraries\n\nimport  numpy as np\nimport  pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn.metrics import accuracy_score \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\nimport warnings\nwarnings.filterwarnings('ignore')"
  },
  {
    "objectID": "posts/kaggle/loan-status-eda-classification.html#importing-data",
    "href": "posts/kaggle/loan-status-eda-classification.html#importing-data",
    "title": "Loan Status Prediction",
    "section": "Importing data",
    "text": "Importing data\n\ntrain = pd.read_csv('../input/loan-prediction-problem-dataset/train_u6lujuX_CVtuZ9i.csv')\ntest = pd.read_csv('../input/loan-prediction-problem-dataset/test_Y3wMUE5_7gLdaTN.csv')\n\n\nprint (train.shape, test.shape)\n\n(614, 13) (367, 12)"
  },
  {
    "objectID": "posts/kaggle/loan-status-eda-classification.html#data-exploration",
    "href": "posts/kaggle/loan-status-eda-classification.html#data-exploration",
    "title": "Loan Status Prediction",
    "section": "Data Exploration",
    "text": "Data Exploration\n\ntrain.head() \n\n\n\n\n\n\n\n\nLoan_ID\nGender\nMarried\nDependents\nEducation\nSelf_Employed\nApplicantIncome\nCoapplicantIncome\nLoanAmount\nLoan_Amount_Term\nCredit_History\nProperty_Area\nLoan_Status\n\n\n\n\n0\nLP001002\nMale\nNo\n0\nGraduate\nNo\n5849\n0.0\nNaN\n360.0\n1.0\nUrban\nY\n\n\n1\nLP001003\nMale\nYes\n1\nGraduate\nNo\n4583\n1508.0\n128.0\n360.0\n1.0\nRural\nN\n\n\n2\nLP001005\nMale\nYes\n0\nGraduate\nYes\n3000\n0.0\n66.0\n360.0\n1.0\nUrban\nY\n\n\n3\nLP001006\nMale\nYes\n0\nNot Graduate\nNo\n2583\n2358.0\n120.0\n360.0\n1.0\nUrban\nY\n\n\n4\nLP001008\nMale\nNo\n0\nGraduate\nNo\n6000\n0.0\n141.0\n360.0\n1.0\nUrban\nY\n\n\n\n\n\n\n\n\ntrain.info() \n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 614 entries, 0 to 613\nData columns (total 13 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   Loan_ID            614 non-null    object \n 1   Gender             601 non-null    object \n 2   Married            611 non-null    object \n 3   Dependents         599 non-null    object \n 4   Education          614 non-null    object \n 5   Self_Employed      582 non-null    object \n 6   ApplicantIncome    614 non-null    int64  \n 7   CoapplicantIncome  614 non-null    float64\n 8   LoanAmount         592 non-null    float64\n 9   Loan_Amount_Term   600 non-null    float64\n 10  Credit_History     564 non-null    float64\n 11  Property_Area      614 non-null    object \n 12  Loan_Status        614 non-null    object \ndtypes: float64(4), int64(1), object(8)\nmemory usage: 62.5+ KB\n\n\n\ntrain.isnull().sum()\n\nLoan_ID               0\nGender               13\nMarried               3\nDependents           15\nEducation             0\nSelf_Employed        32\nApplicantIncome       0\nCoapplicantIncome     0\nLoanAmount           22\nLoan_Amount_Term     14\nCredit_History       50\nProperty_Area         0\nLoan_Status           0\ndtype: int64\n\n\n\ntest.head()\n\n\n\n\n\n\n\n\nLoan_ID\nGender\nMarried\nDependents\nEducation\nSelf_Employed\nApplicantIncome\nCoapplicantIncome\nLoanAmount\nLoan_Amount_Term\nCredit_History\nProperty_Area\n\n\n\n\n0\nLP001015\nMale\nYes\n0\nGraduate\nNo\n5720\n0\n110.0\n360.0\n1.0\nUrban\n\n\n1\nLP001022\nMale\nYes\n1\nGraduate\nNo\n3076\n1500\n126.0\n360.0\n1.0\nUrban\n\n\n2\nLP001031\nMale\nYes\n2\nGraduate\nNo\n5000\n1800\n208.0\n360.0\n1.0\nUrban\n\n\n3\nLP001035\nMale\nYes\n2\nGraduate\nNo\n2340\n2546\n100.0\n360.0\nNaN\nUrban\n\n\n4\nLP001051\nMale\nNo\n0\nNot Graduate\nNo\n3276\n0\n78.0\n360.0\n1.0\nUrban\n\n\n\n\n\n\n\n\ntest.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 367 entries, 0 to 366\nData columns (total 12 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   Loan_ID            367 non-null    object \n 1   Gender             356 non-null    object \n 2   Married            367 non-null    object \n 3   Dependents         357 non-null    object \n 4   Education          367 non-null    object \n 5   Self_Employed      344 non-null    object \n 6   ApplicantIncome    367 non-null    int64  \n 7   CoapplicantIncome  367 non-null    int64  \n 8   LoanAmount         362 non-null    float64\n 9   Loan_Amount_Term   361 non-null    float64\n 10  Credit_History     338 non-null    float64\n 11  Property_Area      367 non-null    object \ndtypes: float64(3), int64(2), object(7)\nmemory usage: 34.5+ KB\n\n\n\ntest.isnull().sum()\n\nLoan_ID               0\nGender               11\nMarried               0\nDependents           10\nEducation             0\nSelf_Employed        23\nApplicantIncome       0\nCoapplicantIncome     0\nLoanAmount            5\nLoan_Amount_Term      6\nCredit_History       29\nProperty_Area         0\ndtype: int64"
  },
  {
    "objectID": "posts/kaggle/loan-status-eda-classification.html#data-preparation-processing",
    "href": "posts/kaggle/loan-status-eda-classification.html#data-preparation-processing",
    "title": "Loan Status Prediction",
    "section": "Data Preparation / Processing",
    "text": "Data Preparation / Processing\n\ndata = [train,test]\nfor dataset in data:\n    #Filter categorical variables\n    categorical_columns = [x for x in dataset.dtypes.index if dataset.dtypes[x]=='object']\n    # Exclude ID cols and source:\n    categorical_columns = [x for x in categorical_columns if x not in ['Loan_ID' ]]\n    #Print frequency of categories\n    \nfor col in categorical_columns:\n    print ('\\nFrequency of Categories for variable %s'%col)\n    print (train[col].value_counts())\n\n\nFrequency of Categories for variable Gender\nGender\nMale      489\nFemale    112\nName: count, dtype: int64\n\nFrequency of Categories for variable Married\nMarried\nYes    398\nNo     213\nName: count, dtype: int64\n\nFrequency of Categories for variable Dependents\nDependents\n0     345\n1     102\n2     101\n3+     51\nName: count, dtype: int64\n\nFrequency of Categories for variable Education\nEducation\nGraduate        480\nNot Graduate    134\nName: count, dtype: int64\n\nFrequency of Categories for variable Self_Employed\nSelf_Employed\nNo     500\nYes     82\nName: count, dtype: int64\n\nFrequency of Categories for variable Property_Area\nProperty_Area\nSemiurban    233\nUrban        202\nRural        179\nName: count, dtype: int64\n\n\n\nGender\n\nsns.countplot(train['Gender'])\n\n\n\n\n\n\n\n\n\npd.crosstab(train.Gender, train.Loan_Status, margins = True)\n\n\n\n\n\n\n\nLoan_Status\nN\nY\nAll\n\n\nGender\n\n\n\n\n\n\n\nFemale\n37\n75\n112\n\n\nMale\n150\n339\n489\n\n\nAll\n187\n414\n601\n\n\n\n\n\n\n\nThe male are in large number as compared to female applicants.Also many of them have positive Loan Status. Further Binarization of this feature should be done,\n\ntrain.Gender = train.Gender.fillna(train.Gender.mode())\ntest.Gender = test.Gender.fillna(test.Gender.mode())\n\nsex = pd.get_dummies(train['Gender'] , drop_first = True )\ntrain.drop(['Gender'], axis = 1 , inplace =True)\ntrain = pd.concat([train , sex ] , axis = 1)\n\nsex = pd.get_dummies(test['Gender'] , drop_first = True )\ntest.drop(['Gender'], axis = 1 , inplace =True)\ntest = pd.concat([test , sex ] , axis = 1)\n\n\n\nDependants\n\nplt.figure(figsize=(6,6))\nlabels = ['0' , '1', '2' , '3+']\nexplode = (0.05, 0, 0, 0)\nsize = [345 , 102 , 101 , 51]\n\nplt.pie(size, explode=explode, labels=labels,\n        autopct='%1.1f%%', shadow = True, startangle = 90)\nplt.axis('equal')\nplt.show()\n\n\n\n\n\n\n\n\n\ntrain.Dependents.value_counts()\n\nDependents\n0     345\n1     102\n2     101\n3+     51\nName: count, dtype: int64\n\n\n\npd.crosstab(train.Dependents , train.Loan_Status, margins = True)\n\n\n\n\n\n\n\nLoan_Status\nN\nY\nAll\n\n\nDependents\n\n\n\n\n\n\n\n0\n107\n238\n345\n\n\n1\n36\n66\n102\n\n\n2\n25\n76\n101\n\n\n3+\n18\n33\n51\n\n\nAll\n186\n413\n599\n\n\n\n\n\n\n\nThe applicants with highest number of dependants are least in number whereas applicants with no dependance are greatest among these.\n\ntrain.Dependents = train.Dependents.fillna(\"0\")\ntest.Dependents = test.Dependents.fillna(\"0\")\n\nrpl = {'0':'0', '1':'1', '2':'2', '3+':'3'}\n\ntrain.Dependents = train.Dependents.replace(rpl).astype(int)\ntest.Dependents = test.Dependents.replace(rpl).astype(int)\n\n\n\nCredit History\n\npd.crosstab(train.Credit_History , train.Loan_Status, margins = True)\n\n\n\n\n\n\n\nLoan_Status\nN\nY\nAll\n\n\nCredit_History\n\n\n\n\n\n\n\n0.0\n82\n7\n89\n\n\n1.0\n97\n378\n475\n\n\nAll\n179\n385\n564\n\n\n\n\n\n\n\n\ntrain.Credit_History = train.Credit_History.fillna(train.Credit_History.mode()[0])\ntest.Credit_History  = test.Credit_History.fillna(test.Credit_History.mode()[0])\n\n\nSelf Employed\n\nsns.countplot(train['Self_Employed'])\n\n\n\n\n\n\n\n\n\npd.crosstab(train.Self_Employed , train.Loan_Status,margins = True)\n\n\n\n\n\n\n\nLoan_Status\nN\nY\nAll\n\n\nSelf_Employed\n\n\n\n\n\n\n\nNo\n157\n343\n500\n\n\nYes\n26\n56\n82\n\n\nAll\n183\n399\n582\n\n\n\n\n\n\n\n\ntrain.Self_Employed = train.Self_Employed.fillna(train.Self_Employed.mode())\ntest.Self_Employed = test.Self_Employed.fillna(test.Self_Employed.mode())\n\nself_Employed = pd.get_dummies(train['Self_Employed'] ,prefix = 'employed' ,drop_first = True )\ntrain.drop(['Self_Employed'], axis = 1 , inplace =True)\ntrain = pd.concat([train , self_Employed ] , axis = 1)\n\nself_Employed = pd.get_dummies(test['Self_Employed'] , prefix = 'employed' ,drop_first = True )\ntest.drop(['Self_Employed'], axis = 1 , inplace =True)\ntest = pd.concat([test , self_Employed ] , axis = 1)\n\n\n\n\nMarried\n\nsns.countplot(train.Married)\n\n\n\n\n\n\n\n\n\npd.crosstab(train.Married , train.Loan_Status,margins = True)\n\n\n\n\n\n\n\nLoan_Status\nN\nY\nAll\n\n\nMarried\n\n\n\n\n\n\n\nNo\n79\n134\n213\n\n\nYes\n113\n285\n398\n\n\nAll\n192\n419\n611\n\n\n\n\n\n\n\n\ntrain.Married = train.Married.fillna(train.Married.mode())\ntest.Married = test.Married.fillna(test.Married.mode())\n\nmarried = pd.get_dummies(train['Married'] , prefix = 'married',drop_first = True )\ntrain.drop(['Married'], axis = 1 , inplace =True)\ntrain = pd.concat([train , married ] , axis = 1)\n\nmarried = pd.get_dummies(test['Married'] , prefix = 'married', drop_first = True )\ntest.drop(['Married'], axis = 1 , inplace =True)\ntest = pd.concat([test , married ] , axis = 1)\n\n\n\nLoan Amount Term and Loan Amount\n\ntrain.drop(['Loan_Amount_Term'], axis = 1 , inplace =True)\ntest.drop(['Loan_Amount_Term'], axis = 1 , inplace =True)\n\ntrain.LoanAmount = train.LoanAmount.fillna(train.LoanAmount.mean()).astype(int)\ntest.LoanAmount = test.LoanAmount.fillna(test.LoanAmount.mean()).astype(int)\n\n\nsns.distplot(train['LoanAmount'])\n\n\n\n\n\n\n\n\nWe observe no outliers in the continuous variable Loan Amount\n\n\nEducation\n\nsns.countplot(train.Education)\n\n\n\n\n\n\n\n\n\ntrain['Education'] = train['Education'].map( {'Graduate': 0, 'Not Graduate': 1} ).astype(int)\ntest['Education'] = test['Education'].map( {'Graduate': 0, 'Not Graduate': 1} ).astype(int)\n\n\n\nProperty Area\n\nsns.countplot(train.Property_Area)\n\n\n\n\n\n\n\n\n\ntrain['Property_Area'] = train['Property_Area'].map( {'Urban': 0, 'Semiurban': 1 ,'Rural': 2  } ).astype(int)\n\ntest.Property_Area = test.Property_Area.fillna(test.Property_Area.mode())\ntest['Property_Area'] = test['Property_Area'].map( {'Urban': 0, 'Semiurban': 1 ,'Rural': 2  } ).astype(int)\n\n\n\nCo-Applicant income and Applicant income\n\nsns.distplot(train['ApplicantIncome'])\n\n\n\n\n\n\n\n\n\nsns.distplot(train['CoapplicantIncome'])\n\n\n\n\n\n\n\n\n\n\nTarget Variable : Loan Status\n\ntrain['Loan_Status'] = train['Loan_Status'].map( {'N': 0, 'Y': 1 } ).astype(int)\n\n\nDropping the ID column\n\ntrain.drop(['Loan_ID'], axis = 1 , inplace =True)"
  },
  {
    "objectID": "posts/kaggle/loan-status-eda-classification.html#view-the-datasets",
    "href": "posts/kaggle/loan-status-eda-classification.html#view-the-datasets",
    "title": "Loan Status Prediction",
    "section": "View the datasets",
    "text": "View the datasets\n\ntrain.head()\n\n\n\n\n\n\n\n\nDependents\nEducation\nApplicantIncome\nCoapplicantIncome\nLoanAmount\nCredit_History\nProperty_Area\nLoan_Status\nMale\nemployed_Yes\nmarried_Yes\n\n\n\n\n0\n0\n0\n5849\n0.0\n146\n1.0\n0\n1\nTrue\nFalse\nFalse\n\n\n1\n1\n0\n4583\n1508.0\n128\n1.0\n2\n0\nTrue\nFalse\nTrue\n\n\n2\n0\n0\n3000\n0.0\n66\n1.0\n0\n1\nTrue\nTrue\nTrue\n\n\n3\n0\n1\n2583\n2358.0\n120\n1.0\n0\n1\nTrue\nFalse\nTrue\n\n\n4\n0\n0\n6000\n0.0\n141\n1.0\n0\n1\nTrue\nFalse\nFalse\n\n\n\n\n\n\n\n\ntest.head()\n\n\n\n\n\n\n\n\nLoan_ID\nDependents\nEducation\nApplicantIncome\nCoapplicantIncome\nLoanAmount\nCredit_History\nProperty_Area\nMale\nemployed_Yes\nmarried_Yes\n\n\n\n\n0\nLP001015\n0\n0\n5720\n0\n110\n1.0\n0\nTrue\nFalse\nTrue\n\n\n1\nLP001022\n1\n0\n3076\n1500\n126\n1.0\n0\nTrue\nFalse\nTrue\n\n\n2\nLP001031\n2\n0\n5000\n1800\n208\n1.0\n0\nTrue\nFalse\nTrue\n\n\n3\nLP001035\n2\n0\n2340\n2546\n100\n1.0\n0\nTrue\nFalse\nTrue\n\n\n4\nLP001051\n0\n1\n3276\n0\n78\n1.0\n0\nTrue\nFalse\nFalse"
  },
  {
    "objectID": "posts/kaggle/loan-status-eda-classification.html#boxplots-for-relation-between-property-area-amount-of-loan-and-education-qualification",
    "href": "posts/kaggle/loan-status-eda-classification.html#boxplots-for-relation-between-property-area-amount-of-loan-and-education-qualification",
    "title": "Loan Status Prediction",
    "section": "Boxplots for relation between Property area, amount of Loan and Education qualification",
    "text": "Boxplots for relation between Property area, amount of Loan and Education qualification\nFurther we analyse the relation between education status,loan taken and property area\n\nProperty_Area:\n\nUrban      :0\nSemiurban  :1\nRural      :2\n\n\n\nplt.figure(figsize=(5,2))\nsns.boxplot(x=\"Property_Area\", y=\"LoanAmount\", hue=\"Education\",data=train, palette=\"coolwarm\")\n\n\n\n\n\n\n\n\n\nThe above boxplot signifies that,\n\nIn the Urban area the non graduates take slightly more loan than graduates.\nIn the Rural and semiurban area the graduates take more amount of Loan than non graduates\nThe higher values of Loan are mostly from Urban area\nThe semiurban area and rural area both have one unusual Loan amount close to zero."
  },
  {
    "objectID": "posts/kaggle/loan-status-eda-classification.html#crosstab-for-relation-between-credit-history-and-loan-status.",
    "href": "posts/kaggle/loan-status-eda-classification.html#crosstab-for-relation-between-credit-history-and-loan-status.",
    "title": "Loan Status Prediction",
    "section": "Crosstab for relation between Credit History and Loan status.",
    "text": "Crosstab for relation between Credit History and Loan status.\n\ntrain.Credit_History.value_counts()\n\nCredit_History\n1.0    525\n0.0     89\nName: count, dtype: int64\n\n\n\nlc = pd.crosstab(train['Credit_History'], train['Loan_Status'])\nlc.plot(kind='bar', stacked=True, color=['red','blue'], grid=False)\n\n\n\n\n\n\n\n\n\nThe credit history vs Loan Status indicates:\n\nThe good credit history applicants have more chances of getting Loan.\nWith better credit History the Loan amount given was greater too.\nBut many were not given loan in the range 0-100\nThe applicant with poor credit history were handled in the range 0-100 only.\n\n\n\nplt.figure(figsize=(9,6))\nsns.heatmap(train.drop('Loan_Status',axis=1).corr(), vmax=0.6, square=True, annot=True)"
  },
  {
    "objectID": "posts/kaggle/loan-status-eda-classification.html#prediction",
    "href": "posts/kaggle/loan-status-eda-classification.html#prediction",
    "title": "Loan Status Prediction",
    "section": "Prediction",
    "text": "Prediction\nThe problem is of Classification as observed and concluded from the data and visualisations.\n\nX = train.drop('Loan_Status' , axis = 1 )\ny = train['Loan_Status']\n\nX_train ,X_test , y_train , y_test = train_test_split(X , y , test_size = 0.3 , random_state =102)\n\n\nfrom sklearn.linear_model import LogisticRegression\nlogmodel = LogisticRegression()\nlogmodel.fit(X_train , y_train)\npred_l = logmodel.predict(X_test)\nacc_l = accuracy_score(y_test , pred_l)*100\nacc_l\n\n83.78378378378379\n\n\n\nrandom_forest = RandomForestClassifier(n_estimators= 100)\nrandom_forest.fit(X_train, y_train)\npred_rf = random_forest.predict(X_test)\nacc_rf = accuracy_score(y_test , pred_rf)*100\nacc_rf\n\n81.08108108108108\n\n\n\nknn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, y_train)\npred_knn = knn.predict(X_test)\nacc_knn = accuracy_score(y_test , pred_knn)*100\nacc_knn\n\n61.08108108108108\n\n\n\ngaussian = GaussianNB()\ngaussian.fit(X_train, y_train)\npred_gb = gaussian.predict(X_test)\nacc_gb = accuracy_score(y_test , pred_gb)*100\nacc_gb\n\n82.16216216216216\n\n\n\nsvc = SVC()\nsvc.fit(X_train, y_train)\npred_svm = svc.predict(X_test)\nacc_svm = accuracy_score(y_test , pred_svm)*100\nacc_svm\n\n70.27027027027027\n\n\n\ngbk = GradientBoostingClassifier()\ngbk.fit(X_train, y_train)\npred_gbc = gbk.predict(X_test)\nacc_gbc = accuracy_score(y_test , pred_gbc)*100\nacc_gbc\n\n82.16216216216216\n\n\n\n## Arranging the Accuracy results\nmodels = pd.DataFrame({\n    'Model': ['Logistic Regression', 'Random Forrest','K- Nearest Neighbour' ,\n             'Naive Bayes' , 'SVM','Gradient Boosting Classifier'],\n    'Score': [acc_l , acc_rf , acc_knn , acc_gb ,acc_svm ,acc_gbc ]})\nmodels.sort_values(by='Score', ascending=False)\n\n\n\n\n\n\n\n\nModel\nScore\n\n\n\n\n0\nLogistic Regression\n83.783784\n\n\n3\nNaive Bayes\n82.162162\n\n\n5\nGradient Boosting Classifier\n82.162162\n\n\n1\nRandom Forrest\n81.081081\n\n\n4\nSVM\n70.270270\n\n\n2\nK- Nearest Neighbour\n61.081081\n\n\n\n\n\n\n\nThe highest classification accuracy is shown by Logistic Regression of about 83.24 %\nLet us Check th feature importance,\n\nimportances = pd.DataFrame({'Features':X_train.columns,'Importance':np.round(random_forest.feature_importances_,3)})\nimportances = importances.sort_values('Importance',ascending=False).set_index('Features')\nimportances.head(11) \n\n\n\n\n\n\n\n\nImportance\n\n\nFeatures\n\n\n\n\n\nCredit_History\n0.235\n\n\nApplicantIncome\n0.219\n\n\nLoanAmount\n0.211\n\n\nCoapplicantIncome\n0.125\n\n\nDependents\n0.057\n\n\nProperty_Area\n0.051\n\n\nmarried_Yes\n0.029\n\n\nEducation\n0.028\n\n\nMale\n0.026\n\n\nemployed_Yes\n0.020\n\n\n\n\n\n\n\n\nimportances.plot.bar()\n\n\n\n\n\n\n\n\nCredit History has the maximum importance and empoloyment has the least!\n\nSummarizing\nThe Loan status has better relation with features such as Credit History, Applicant’s Income, Loan Amount needed by them, Family status(Depenedents) and Property Area which are generally considered by the loan providing organisations. These factors are hence used to take correct decisions to provide loan status or not. This data analysis hence gives a realisation of features and the relation between them from the older decision examples hence giving a learning to predict the class of the unseen data.\nFinally the we predict over unseen dataset using the Logistic Regression and Random Forest model(Ensemble Learning):\n\ndf_test = test.drop(['Loan_ID'], axis = 1)\n\n\ndf_test.head()\n\n\n\n\n\n\n\n\nDependents\nEducation\nApplicantIncome\nCoapplicantIncome\nLoanAmount\nCredit_History\nProperty_Area\nMale\nemployed_Yes\nmarried_Yes\n\n\n\n\n0\n0\n0\n5720\n0\n110\n1.0\n0\nTrue\nFalse\nTrue\n\n\n1\n1\n0\n3076\n1500\n126\n1.0\n0\nTrue\nFalse\nTrue\n\n\n2\n2\n0\n5000\n1800\n208\n1.0\n0\nTrue\nFalse\nTrue\n\n\n3\n2\n0\n2340\n2546\n100\n1.0\n0\nTrue\nFalse\nTrue\n\n\n4\n0\n1\n3276\n0\n78\n1.0\n0\nTrue\nFalse\nFalse\n\n\n\n\n\n\n\n\np_log = logmodel.predict(df_test)\n\n\np_rf = random_forest.predict(df_test)\n\n\npredict_combine = np.zeros((df_test.shape[0]))\n\nfor i in range(0, test.shape[0]):\n    temp = p_log[i] + p_rf[i]\n    if temp&gt;=2:\n        predict_combine[i] = 1\npredict_combine = predict_combine.astype('int')\n\n\nsubmission = pd.DataFrame({\n        \"Loan_ID\": test[\"Loan_ID\"],\n        \"Loan_Status\": predict_combine\n    })\n\nsubmission.to_csv(\"results.csv\", encoding='utf-8', index=False)"
  }
]